{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwu_measures\n",
    "import pandas as pd\n",
    "import mwu_measures.preprocessing_corpus\n",
    "import mwu_measures.processing_corpus\n",
    "from mwu_measures.corpus_helper import Fetcher\n",
    "from mwu_measures.corpus import Corpus\n",
    "import duckdb\n",
    "from rich.progress import Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mwu_examples = pd.read_csv('MultiwordExpression_Concreteness_Ratings.csv')\n",
    "mwu_examples['length'] = mwu_examples['Expression'].apply(lambda x: len(x.split()))\n",
    "mwu_examples = mwu_examples.loc[(mwu_examples['length'] == 2) | (mwu_examples['length'] == 3)]\n",
    "mwu_examples['Expression'] = mwu_examples['Expression'].apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using preexisting corpus\n"
     ]
    }
   ],
   "source": [
    "# mwu_measures.processing_corpus.make_processed_corpus('coca_fourgrams', 'mwu_measures/corpora/coca_texts/', chunk_size=30, verbose=True, threshold=2)\n",
    "test_corpus = Corpus(\"coca_fourgrams\")\n",
    "helper = Fetcher(test_corpus)\n",
    "\n",
    "# wrapper = lp(mwu_measures.processing_corpus.get_processed_corpus)\n",
    "# this_corpus = mwu_measures.processing_corpus.get_processed_corpus('bnc', 'small_corpus.txt', chunk_size=10000000, verbose=False)\n",
    "# this_corpus = mwu_measures.processing_corpus.make_processed_corpus(test_corpus=True, threshold=0)\n",
    "# lp.print_stats()\n",
    "# ngram_selection = [ngram.split() for ngram in ngram_selection]\n",
    "# ngram_chunks = np.array_split(ngram_selection, 100)\n",
    "# helper = Fetcher(this_corpus)\n",
    "# 340 seconds for all _acad_. Not bad, not the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trigram_db_temp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unigram_db_temp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name\n",
       "0  trigram_db_temp\n",
       "1  unigram_db_temp"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus.df(\"SHOW TABLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c609e880e6544b77ac08ca4c19d418ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d550ae62c04e7980536bd445721b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0b67299bc94847bf8adeeb34550ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9ea4a47338474b80b30cd038059353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_corpus.consolidate_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus.create_totals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Hello! Come in and eat boiled potatoes they are one hundred percent good.\\nI remember when we met in the year 2000'\n",
    "sentence = mwu_measures.process_text(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello come',\n",
       " 'hello come in',\n",
       " 'hello come in and',\n",
       " 'come in',\n",
       " 'come in and',\n",
       " 'come in and eat',\n",
       " 'in and',\n",
       " 'in and eat',\n",
       " 'in and eat boiled',\n",
       " 'and eat',\n",
       " 'and eat boiled',\n",
       " 'and eat boiled potatoes',\n",
       " 'eat boiled',\n",
       " 'eat boiled potatoes',\n",
       " 'eat boiled potatoes they',\n",
       " 'boiled potatoes',\n",
       " 'boiled potatoes they',\n",
       " 'boiled potatoes they are',\n",
       " 'potatoes they',\n",
       " 'potatoes they are',\n",
       " 'potatoes they are one',\n",
       " 'they are',\n",
       " 'they are one',\n",
       " 'they are one hundred',\n",
       " 'are one',\n",
       " 'are one hundred',\n",
       " 'are one hundred percent',\n",
       " 'one hundred',\n",
       " 'one hundred percent',\n",
       " 'one hundred percent good',\n",
       " 'hundred percent',\n",
       " 'hundred percent good',\n",
       " 'percent good',\n",
       " 'i remember',\n",
       " 'i remember when',\n",
       " 'i remember when we',\n",
       " 'remember when',\n",
       " 'remember when we',\n",
       " 'remember when we met',\n",
       " 'when we',\n",
       " 'when we met',\n",
       " 'when we met in',\n",
       " 'we met',\n",
       " 'we met in',\n",
       " 'we met in the',\n",
       " 'met in',\n",
       " 'met in the',\n",
       " 'met in the year',\n",
       " 'in the',\n",
       " 'in the year',\n",
       " 'in the year NUMBER',\n",
       " 'the year',\n",
       " 'the year NUMBER',\n",
       " 'year NUMBER']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d640b60df5444dcdbcfec15f09e8cf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1676312d1b814339b35470ba478585fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bd2d444ab34423b4daee3b99ba2fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fdaa5ec77af4da5a06d780c63db8901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7532eca907d84685b882ad38686e80fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = helper.get_score_batch(sentence, from_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['raw'][x['raw'].ngram_length == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures.processing_corpus.make_processed_corpus('test', test_corpus=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = Corpus(\"test\")\n",
    "helper = Fetcher(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [['b', 'd'], ['c', 'b'], ['a', 'c']]\n",
    "test_corpus.create_query(bigrams, 'ug_1', 'ug_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus.get_ngram_scores('ug_1', 'ug_2', 2)['raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_corpus = Corpus(\"coca\")\n",
    "helper = Fetcher(this_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = mwu_examples['Expression'].tolist()\n",
    "# ngrams = [[ngram[0] + ' ' + ngram[1], ngram[2]] if len(ngram) == 3 else ngram for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = helper.get_score_batch(ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x[0][~x[0]['ngram_length'].isna()].drop_duplicates().sort_values(by=['mwu_score'], ascending=False).iloc[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data_1 = pd.read_csv('test.csv', usecols=['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'])\n",
    "human_data_2 = pd.read_csv('train.csv', usecols=['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'])\n",
    "human_data_3 = pd.read_csv('valid.csv', usecols=['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'])\n",
    "human_data = pd.concat([human_data_1, human_data_2, human_data_3]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data['utterance'] = human_data['utterance'].str.replace('_comma_', ' , ')\n",
    "human_data['prompt'] = human_data['prompt'].str.replace('_comma_', ' , ')\n",
    "human_data['utterance'] = human_data['utterance'].str.replace('n t ', \"n't \", regex=False)\n",
    "human_data['prompt'] = human_data['prompt'].str.replace('n t ', \"n't \", regex=False)\n",
    "human_data['utterance'] = human_data['utterance'].str.replace(r'n t[$\\.]', \"n't\", regex=True)\n",
    "human_data['prompt'] = human_data['prompt'].str.replace(r'n t[$\\.]', \"n't\", regex=True)\n",
    "# human_data['prompt'] = human_data['prompt'].str.replace('wasnt', \"wasn't\", regex=True)\n",
    "# human_data['utterance'] = human_data['utterance'].str.replace('wasnt', \"wasn't\", regex=True)\n",
    "\n",
    "human_utterances = human_data.utterance.apply(mwu_measures.process_text)\n",
    "human_prompts = human_data.prompt.apply(mwu_measures.process_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = pd.concat([human_utterances, human_prompts])\n",
    "human_text = human_text.explode()\n",
    "human_text = human_text.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = human_text.drop_duplicates().dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batches = [human_text.iloc[i*50000:(i+1)*50000].copy() for i in range(int(len(human_text) / 50000) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batches = [batch.to_list() for batch in text_batches]\n",
    "# TODO: does wasn t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weights = {'token_freq': 1/8, 'dispersion': 1/4, 'type_1': 1/16, 'type_2': 1/8, 'entropy_1': 1/16, 'entropy_2': 1/8, 'fw_assoc': 1/8, 'bw_assoc': 1/16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text_mwu = []\n",
    "i = 0\n",
    "for batch in text_batches:\n",
    "    print((i / len(text_batches)))\n",
    "    human_text_mwu.append(helper.get_score_batch(batch, weights=test_weights, from_text=True))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text_mwu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((result[0] for result in human_text_mwu)).sort_values(by = 'ngram').to_csv('human_mwu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text_mwu.drop_duplicates().sort_values(by=['mwu_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LO HICE!!!\n",
    "this_corpus.create_query(ngrams, 'big_1', 'ug_3')\n",
    "this_corpus.get_ngram_scores('big_1', 'ug_3', 3, [-0.1, 0.1])['normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_corpus.df(\"SELECT * FROM trigram_db WHERE big_1 = HASH('read up') AND ug_3 = HASH('about')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper(\"SELECT DISTINCT * FROM entropy_diffs\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy broken for trigrams again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = helper.get_score_batch(mwu_examples['Expression']).sort_values(by='mwu_score')\n",
    "x[x['ngram_length'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper(\"SELECT * FROM raw_measures\", df = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = ['come in handy', 'family friend', 'database management system', 'one hundred percent', 'line of control', 'like a', 'boiled potatoes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = helper\n",
    "normalized=True\n",
    "# self.get_score_batch(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.create_scores(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.get_score_batch(sentence)\n",
    "# TODO: why are they repeated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_corpus.df(\"VACUUM ANALYZE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rewrite documentation and merge onto main. This is basically ready.\n",
    "# TODO: simplify process_text flowline. default for helper?? process > create > return? > clean?\n",
    "# TODO: Brown as default corpus.\n",
    "# > this for after cogsci\n",
    "# TODO: 4-grams. Should need minimal modification. Challenge might be RAM. Consider implementing an option to work from disk with duckdb?\n",
    "# > This for after cogsci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data = pd.read_csv('2GPTEmpathicDialoguesDataset (1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_utterances = gpt_data.processed.apply(mwu_measures.process_text)\n",
    "gpt_prompts = gpt_data.prompt.apply(mwu_measures.process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_text = pd.concat([gpt_utterances, gpt_prompts])\n",
    "gpt_text = gpt_text.explode()\n",
    "gpt_text = gpt_text.dropna()\n",
    "gpt_text = gpt_text.drop_duplicates().dropna().reset_index(drop=True)\n",
    "text_batches = [gpt_text.iloc[i*50000:(i+1)*50000].copy() for i in range(int(len(gpt_text) / 50000) + 1)]\n",
    "text_batches = [batch.to_list() for batch in text_batches]\n",
    "nice_weights = {'token_freq': 1/8, 'dispersion': 1/4, 'type_1': 1/16, 'type_2': 1/8, 'entropy_1': 1/16, 'entropy_2': 1/8, 'fw_assoc': 1/8, 'bw_assoc': 1/16}\n",
    "gpt_text_mwu = []\n",
    "i = 0\n",
    "for batch in text_batches:\n",
    "    print(round(i / len(text_batches), 2))\n",
    "    gpt_text_mwu.append(helper.get_score_batch(batch, weights=nice_weights, from_text=True))\n",
    "    i += 1\n",
    "\n",
    "pd.concat((result[0] for result in gpt_text_mwu)).sort_values(by = 'ngram').to_csv('gpt_mwu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data_1 = pd.read_csv('test.csv', usecols=['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'])\n",
    "human_data_2 = pd.read_csv('train.csv', usecols=['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'])\n",
    "human_data_3 = pd.read_csv('valid.csv', usecols=['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'])\n",
    "human_data = pd.concat([human_data_1, human_data_2, human_data_3]).reset_index(drop=True)\n",
    "human_data['utterance'] = human_data['utterance'].str.replace('_comma_', ' , ')\n",
    "human_data['prompt'] = human_data['prompt'].str.replace('_comma_', ' , ')\n",
    "human_data['utterance'] = human_data['utterance'].str.replace('n t ', \"n't \", regex=False)\n",
    "human_data['prompt'] = human_data['prompt'].str.replace('n t ', \"n't \", regex=False)\n",
    "human_data['utterance'] = human_data['utterance'].str.replace(r'n t[$\\.]', \"n't\", regex=True)\n",
    "human_data['prompt'] = human_data['prompt'].str.replace(r'n t[$\\.]', \"n't\", regex=True)\n",
    "# human_data['prompt'] = human_data['prompt'].str.replace('wasnt', \"wasn't\", regex=True)\n",
    "# human_data['utterance'] = human_data['utterance'].str.replace('wasnt', \"wasn't\", regex=True)\n",
    "\n",
    "human_data['utterance'] = human_data.utterance.apply(mwu_measures.process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_scores = pd.read_csv('human_mwu_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_scores = dict(zip(human_scores['ngram'].to_list(), human_scores['mwu_score'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data = human_data.explode('utterance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data = human_data[~human_data['utterance'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data['mwu_score'] = human_data.utterance.apply(lambda x: human_scores[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data = human_data.reset_index(drop=True)\n",
    "human_data['trigram_id'] = human_data.index // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data[['conv_id', 'utterance_idx', 'prompt', 'speaker_idx', 'utterance', 'mwu_score', 'trigram_id']].to_csv('human_trigram_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data = pd.read_csv('2GPTEmpathicDialoguesDataset (1).csv')\n",
    "gpt_data['utterance'] = gpt_data.processed.apply(mwu_measures.process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_scores = pd.read_csv('gpt_mwu_scores.csv')\n",
    "gpt_scores = dict(zip(gpt_scores['ngram'].to_list(), gpt_scores['mwu_score'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data = gpt_data.explode('utterance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data = gpt_data[~gpt_data['utterance'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data['mwu_score'] = gpt_data.utterance.apply(lambda x: gpt_scores[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data = gpt_data.reset_index(drop=True)\n",
    "gpt_data['trigram_id'] = gpt_data.index // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data[['conv_id', 'prompt', 'utterance', 'mwu_score', 'trigram_id']].to_csv('gpt_trigram_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mwu_measures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
