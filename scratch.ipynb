{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text_acad_1990.txt', 'text_acad_1991.txt', 'text_blog_18.txt']\n",
      "4732\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 7.38525 s\n",
      "File: /home/pablo/Documents/GitHub/MWU_measures/mwu_measures/preprocessing_corpus.py\n",
      "Function: clean_coca_lines at line 35\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    35                                           def clean_coca_lines(raw_lines, corpus_ids):\n",
      "    36         1     909467.0 909467.0      0.0      raw_lines = pd.DataFrame({'lines': raw_lines, 'ids': corpus_ids})\n",
      "    37         1 2055757278.0    2e+09     27.8      raw_lines['lines'] = raw_lines['lines'].str.replace(r'(?:@ )+| ?</?[ph]> ?| ?<br> ? | [\\.\\?\\!] ', '#----SPLIT----#', regex=True)\n",
      "    38         1   29185994.0    3e+07      0.4      raw_lines['lines'] = raw_lines['lines'].str.lower()\n",
      "    39         1  423573363.0    4e+08      5.7      raw_lines['lines'] = raw_lines['lines'].str.replace(r' \\n| &gt;|@@\\d+', '', regex=True)\n",
      "    40         1  241413427.0    2e+08      3.3      raw_lines['lines'] = raw_lines['lines'].str.replace(r\" (n't|'s|'ll|'d|'re|'ve|'m)\", r\"\\1\", regex=True)\n",
      "    41         1   39348916.0    4e+07      0.5      raw_lines['lines'] = raw_lines['lines'].str.replace('wan na', 'wanna', regex = False)\n",
      "    42         1   74254419.0    7e+07      1.0      raw_lines['lines'] = raw_lines['lines'].str.replace('-', '', regex=False)\n",
      "    43         1 1276827400.0    1e+09     17.3      raw_lines['lines'] = raw_lines['lines'].str.replace(r'\\s\\d+\\s|^\\d+\\s|\\s\\d+$', ' NUMBER ', regex=True)\n",
      "    44         1   17067599.0    2e+07      0.2      raw_lines['lines'] = raw_lines['lines'].str.strip()\n",
      "    45         1 2137426472.0    2e+09     28.9      raw_lines['lines'] = raw_lines['lines'].str.replace(r'\\s+\\W\\s+|\\s+\\W$|\\W\\s+$', ' ', regex=True)\n",
      "    46         1    1590684.0    2e+06      0.0      raw_lines['lines'] = raw_lines['lines'].str.strip()\n",
      "    47         1 1054309212.0    1e+09     14.3      raw_lines['lines'] = raw_lines['lines'].str.replace(r'\\s+', ' ', regex=True)\n",
      "    48         1    1452471.0    1e+06      0.0      raw_lines['lines'] = raw_lines['lines'][raw_lines['lines'].str.len() > 0]\n",
      "    49         1     986494.0 986494.0      0.0      raw_lines['lines'] = raw_lines['lines'].str.strip()\n",
      "    50         1   13684420.0    1e+07      0.2      raw_lines['lines'] = raw_lines['lines'].str.split('#----SPLIT----#', regex=False)\n",
      "    51         1    2466246.0    2e+06      0.0      raw_lines = raw_lines.explode('lines', ignore_index=True) # Splits paragraphs, lines, and sentences\n",
      "    52         1     913692.0 913692.0      0.0      processed_lines = raw_lines['lines'].str.strip()\n",
      "    53         1   13661367.0    1e+07      0.2      processed_lines = 'START ' + processed_lines + ' END'\n",
      "    54                                           \n",
      "    55                                               # processed_lines = processed_lines.str.lower()\n",
      "    56                                               # processed_lines = processed_lines.str.replace(r' \\n| &gt;|@@\\d+', '', regex=True)\n",
      "    57                                               # processed_lines = processed_lines.str.replace(r\" (n't|'s|'ll|'d|'re|'ve|'m)\", r\"\\1\", regex=True)\n",
      "    58                                               # processed_lines = processed_lines.str.replace('wan na', 'wanna', regex = False)\n",
      "    59                                               # processed_lines = processed_lines.str.replace('-', '', regex=False)\n",
      "    60                                               # processed_lines = processed_lines.str.replace(r'\\s\\d+\\s|^\\d+\\s|\\s\\d+$', ' NUMBER ', regex=True)\n",
      "    61                                               # processed_lines = processed_lines.str.strip()\n",
      "    62                                               # processed_lines = processed_lines.str.replace(r'\\s+\\W\\s+|\\s+\\W$|\\W\\s+$', ' ', regex=True)\n",
      "    63                                               # processed_lines = processed_lines.str.strip()\n",
      "    64                                               # processed_lines = processed_lines.str.replace(r'\\s+', ' ', regex=True)\n",
      "    65                                               # processed_lines = processed_lines[processed_lines.str.len() > 0]\n",
      "    66                                               # processed_lines = 'START ' + processed_lines + ' END'\n",
      "    67         1     417577.0 417577.0      0.0      return list(zip(raw_lines['ids'], processed_lines))\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/pablo/Documents/GitHub/MWU_measures/scratch.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n",
      "File \u001b[0;32m~/Documents/GitHub/MWU_measures/mwu_measures/processing_corpus.py:80\u001b[0m, in \u001b[0;36mget_processed_corpus\u001b[0;34m(corpus_name, corpus_dir, verbose, test_corpus, chunk_size)\u001b[0m\n\u001b[1;32m     78\u001b[0m         ids_chunk\u001b[39m.\u001b[39mextend([corpus_ids[coca_text]] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(raw_lines))\n\u001b[1;32m     79\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(lines_chunk))\n\u001b[0;32m---> 80\u001b[0m ngram_dicts \u001b[39m=\u001b[39m preprocessing_corpus\u001b[39m.\u001b[39mpreprocess_corpus(raw_lines\u001b[39m=\u001b[39mlines_chunk, corpus\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoca\u001b[39m\u001b[39m'\u001b[39m, corpus_ids\u001b[39m=\u001b[39mids_chunk)\n\u001b[1;32m     81\u001b[0m this_corpus\u001b[39m.\u001b[39madd_chunk(ngram_dicts)\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Documents/GitHub/MWU_measures/mwu_measures/preprocessing_corpus.py:103\u001b[0m, in \u001b[0;36mpreprocess_corpus\u001b[0;34m(raw_lines, corpus, corpus_ids)\u001b[0m\n\u001b[1;32m    101\u001b[0m org_items \u001b[39m=\u001b[39m groupby(this_lines, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m0\u001b[39m])\n\u001b[1;32m    102\u001b[0m unigrams \u001b[39m=\u001b[39m {key:[line[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m group] \u001b[39mfor\u001b[39;00m key, group \u001b[39min\u001b[39;00m org_items}\n\u001b[0;32m--> 103\u001b[0m trigrams \u001b[39m=\u001b[39m {key: Counter([trigram \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m group \u001b[39mfor\u001b[39;00m trigram \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(generate_trigrams(line))]) \u001b[39mfor\u001b[39;00m key, group \u001b[39min\u001b[39;00m unigrams\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    104\u001b[0m trigrams \u001b[39m=\u001b[39m [(corpus, \u001b[39m*\u001b[39mngram, freq) \u001b[39mfor\u001b[39;00m corpus, corpus_dict \u001b[39min\u001b[39;00m trigrams\u001b[39m.\u001b[39mitems() \u001b[39mfor\u001b[39;00m ngram, freq \u001b[39min\u001b[39;00m corpus_dict\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m    105\u001b[0m unigrams \u001b[39m=\u001b[39m {corpus: Counter([unigram \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m corpus_lines \u001b[39mfor\u001b[39;00m unigram \u001b[39min\u001b[39;00m line\u001b[39m.\u001b[39msplit()]) \u001b[39mfor\u001b[39;00m corpus, corpus_lines \u001b[39min\u001b[39;00m unigrams\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/Documents/GitHub/MWU_measures/mwu_measures/preprocessing_corpus.py:74\u001b[0m, in \u001b[0;36mgenerate_trigrams\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_trigrams\u001b[39m(text):\n\u001b[0;32m---> 74\u001b[0m     words \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39msplit()\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(words, islice(words, \u001b[39m1\u001b[39m, \u001b[39mNone\u001b[39;00m), islice(words, \u001b[39m2\u001b[39m, \u001b[39mNone\u001b[39;00m)))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import mwu_measures\n",
    "import pandas as pd\n",
    "import mwu_measures.processing_corpus\n",
    "from mwu_measures.corpus_helper import Fetcher\n",
    "mwu_examples = pd.read_csv('MultiwordExpression_Concreteness_Ratings.csv')\n",
    "mwu_examples['length'] = mwu_examples['Expression'].apply(lambda x: len(x.split()))\n",
    "mwu_examples = mwu_examples.loc[(mwu_examples['length'] == 2) | (mwu_examples['length'] == 3)]\n",
    "mwu_examples['Expression'] = mwu_examples['Expression'].apply(lambda x: x.lower())\n",
    "# this_corpus = mwu_measures.processing_corpus.get_processed_corpus('bnc', 'small_corpus.txt', chunk_size=10000000, verbose=False)\n",
    "this_corpus = mwu_measures.processing_corpus.get_processed_corpus('coca', 'mwu_measures/corpora/coca_sample/', chunk_size=3, verbose=True)\n",
    "\n",
    "# this_corpus = mwu_measures.processing_corpus.process_corpus(test_corpus=True)\n",
    "\n",
    "# ngram_selection = [ngram.split() for ngram in ngram_selection]\n",
    "# ngram_chunks = np.array_split(ngram_selection, 100)\n",
    "\n",
    "# TODO WARNING unaggregated table uses A LOT of ram. Aggregate after each chunk.\n",
    "# TODO: regex cleaning is slowing everything down. Problem seems to be on excessive wildcard use. Benchmark against simplified patterns?\n",
    "# bigrams = [['b', 'd'], ['c', 'b'], ['a', 'c']]\n",
    "\n",
    "helper = Fetcher(this_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = mwu_examples['Expression'].tolist()\n",
    "helper.create_scores(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weights = {'token_freq': 1/8, 'dispersion': 1/4, 'type_1': 1/16, 'type_2': 1/16, 'entropy_1': 1/16, 'entropy_2': 1/16, 'fw_assoc': 1/8, 'bw_assoc': 1/4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.get_score_batch(mwu_examples['Expression']).sort_values(by='mwu_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = ['come in handy', 'family friend', 'database management system', 'one hundred percent', 'line of control', 'like a', 'boiled potatoes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = helper\n",
    "normalized=True\n",
    "self.get_score_batch(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence = 'Hello! Come in and eat boiled potatoes they are one hundred percent good.\\nI remember when we met in the year 2000'\n",
    "sentence = mwu_measures.process_text(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.create_scores(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.get_score_batch(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rewrite documentation and merge onto main. This is basically ready.\n",
    "# TODO: CoCA corpus support. GOOD..\n",
    "# TODO: Brown as default corpus.\n",
    "# TODO: 4-grams. Should need minimal modification. Challenge might be RAM. Consider implementing an option to work from disk with duckdb?\n",
    "# TODO: batches to not crash memory when using complete corpus. Worth it? faster because of the simpler joins? worth it to hash? Maybe a way to add rows to a table? Test whether it crashes with a long list of ngrams or with CoCA in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "coca_dir = 'mwu_measures/corpora/coca_texts/'\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "#from mwu_measures.preprocessing_corpus import preprocess_corpus\n",
    "from mwu_measures.preprocessing_corpus import clean_coca_lines\n",
    "\n",
    "coca_texts = sorted(os.listdir(coca_dir))\n",
    "corpus_ids = dict(zip(sorted(coca_texts), range(len(coca_texts))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_chunks = [coca_texts[i:i+20] for i in range(0, len(coca_texts), 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coca_chunk in coca_chunks:\n",
    "    lines_chunk = []\n",
    "    ids_chunk = []\n",
    "    for coca_text in coca_chunk:\n",
    "        with open(os.path.join(coca_dir, coca_text)) as corpus_file:\n",
    "            raw_lines = corpus_file.readlines()\n",
    "            line_chunks.extend(raw_lines)\n",
    "            id_chunks.extend([corpus_ids[coca_text]] * len(raw_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame({'lines': line_chunks, 'ids': id_chunks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['split_lines'] = x['lines'].str.split(r'(?:@ )+| ?</?[ph]> ?| ?<br> ?', regex=True)\n",
    "x = x.explode('split_lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.explode('lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mwu_measures.preprocessing_corpus import preprocess_corpus\n",
    "preprocess_corpus(raw_lines, 'coca', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_lines = pd.Series(raw_lines).str.split(r'(?:@ )+| ?</?[ph]> ?| ?<br> ? | [\\.\\?\\!] ', regex=True).explode(ignore_index=True) # Splits paragraphs, lines, and sentences\n",
    "raw_lines = raw_lines.str.strip()\n",
    "raw_lines = raw_lines.str.lower()\n",
    "raw_lines = raw_lines.str.replace(r' \\n| &gt;|@@\\d+', '', regex=True)\n",
    "raw_lines = raw_lines.str.replace(r\" (n't|'s|'ll|'d|'re|'ve|'m)\", r\"\\1\", regex=True)\n",
    "raw_lines = raw_lines.str.replace('wan na', 'wanna', regex = False)\n",
    "raw_lines = raw_lines.str.replace('-', '', regex=False)\n",
    "raw_lines = raw_lines.str.replace(r'\\s\\d+\\s|^\\d+\\s|\\s\\d+$', ' NUMBER ', regex=True)\n",
    "raw_lines = raw_lines.str.strip()\n",
    "raw_lines = raw_lines.str.replace(r'\\s+\\W\\s+|\\s+\\W$|\\W\\s+$', ' ', regex=True)\n",
    "raw_lines = raw_lines.str.strip()\n",
    "raw_lines = raw_lines.str.replace(r'\\s+', ' ', regex=True)\n",
    "raw_lines = raw_lines[raw_lines.str.len() > 0]\n",
    "raw_lines = 'START ' + raw_lines + ' END'\n",
    "lines = list(zip(itertools.cycle([corpus_ids[this_corpus]]), raw_lines.to_list(), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    y = x.str.replace(r'^.+\\t', '', regex=True)\n",
    "    y = y.str.lower()\n",
    "    y = y.str.replace(r\" (n't|'s|'ll|'d|'re|'ve|'m)\", r\"\\1\", regex=True)\n",
    "    y = y.str.replace('wan na', 'wanna', regex = False)\n",
    "    y = y.str.replace('\\n', '')\n",
    "    y = y.str.replace('-', '')\n",
    "    y = y.str.replace(r'\\s\\d+\\s|^\\d+\\s|\\s\\d+$', ' NUMBER ', regex=True)\n",
    "    y = y.str.strip()\n",
    "    y = y.str.replace(r'\\s*\\W+\\s*', ' ', regex=True)\n",
    "    y = y.str.strip()\n",
    "    y = y.str.replace(r'\\s+', ' ', regex=True)\n",
    "    y = 'START ' + y + ' END'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete:  \\n| &gt;|@@\\d+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(, raw_lines[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mwu_measures/corpora/coca_texts/', 'r', encoding=\"utf-8\") as corpus_file:\n",
    "    i = 0\n",
    "    while True:\n",
    "        raw_lines = corpus_file.readlines(chunk_size)\n",
    "        if not raw_lines:\n",
    "            break\n",
    "        ngram_dicts = preprocessing_corpus.preprocess_bnc(raw_lines)\n",
    "        this_corpus.add_chunk(ngram_dicts)\n",
    "        if verbose:\n",
    "            i += len(raw_lines)\n",
    "            print(f'{i} lines processed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mwu_measures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
