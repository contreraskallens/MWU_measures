{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import re\n",
    "# from collections import defaultdict, Counter\n",
    "# # from itertools import groupby\n",
    "# # from nltk import flatten\n",
    "# # from nltk.util import trigrams as get_trigrams\n",
    "# from mwu_measures.preprocessing_corpus import clean_bnc_line, preprocess_bnc\n",
    "# # import mwu_measures\n",
    "# from mwu_measures.processing_corpus import Corpus\n",
    "import mwu_measures\n",
    "from mwu_measures.compute_functions import min_max_norm\n",
    "from mwu_measures import compute_functions\n",
    "from mwu_measures import processing_corpus\n",
    "# from mwu_measures.mwu_functions import get_association, get_entropy_dif\n",
    "from collections import defaultdict, Counter\n",
    "# from nltk import FreqDist\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dispersion(ngram_freq, token_freq, corpus_proportions):\n",
    "    \"\"\"\n",
    "    Computes the \"Dispersion\" variable for an ngram as the\n",
    "    KLD divergence between its occurrences in each corpus and\n",
    "    the overall corpus proportions.\n",
    "    :param bigram_freq: The frequency of the ngram in each\n",
    "        corpus as a (corpus, frequency) tuple.\n",
    "    :param token_freq: The token frequency of the ngram,\n",
    "        used to transform frequencies into proportions.\n",
    "    :returns: Dispersion measure as a scalar.\n",
    "    \"\"\"\n",
    "    \n",
    "    ngram_props = [(corpus, freq / token_freq) for corpus, freq in ngram_freq.items()]\n",
    "    ngram_props = pd.DataFrame(ngram_props, columns=['corpus', 'ngram_prop'])\n",
    "    ngram_props = pd.merge(\n",
    "        corpus_proportions,\n",
    "        ngram_props,\n",
    "        on='corpus',\n",
    "        how='left'\n",
    "        ).fillna(0)\n",
    "    kld_props = compute_functions.get_kld(ngram_props['ngram_prop'].values,\n",
    "        ngram_props['freq'].values)\n",
    "    return kld_props\n",
    "\n",
    "def get_association(part_1, part_2, token_freq, unigram_frequencies, fw_dist, n_trigrams=None):\n",
    "    \"\"\"\n",
    "    Obtains the association between the components of a bigram as\n",
    "    the KLD between joint occurrence and overall occurrence.\n",
    "    Calculated both forward and backwards in the ngram.\n",
    "    :param comp_1: String with the first component of the ngram. Tuple for trigram association.\n",
    "    :param comp_2: String with the second component of the ngram.\n",
    "    :param token_freq: Token frequency of the ngram.\n",
    "    :unigram_frequencies: Overall unigram frequencies, summed \n",
    "        over the whole corpus. In the form of a Counter.\n",
    "    :param bigram_frequencies: Necessary only for backwards association in \n",
    "        trigrams. In the form of {corpus: {a: {b {c: x}}}}.\n",
    "    :return: A tuple, (association_forward, association_backward).\n",
    "    \"\"\"\n",
    "    # unigram_frequencies = processing_corpus.UNIGRAM_TOTAL\n",
    "    # joint probability is conditioned on the unigram frequencies\n",
    "\n",
    "    comp_1_freq = fw_dist.total()\n",
    "    comp_2_freq = unigram_frequencies.get(part_2, 0) # Part 2 is a unigram in bigrams and trigrams\n",
    "    prob_2_1 = token_freq / comp_1_freq\n",
    "    prob_1_2 = token_freq / comp_2_freq\n",
    "\n",
    "    if isinstance(part_1, tuple):\n",
    "        prob_1 = comp_1_freq / n_trigrams # Because the frequency is calculated by taking summing over trigram frequencies, the probability should be calculated with those too.\n",
    "    else: \n",
    "        prob_1 = comp_1_freq / unigram_frequencies.total()\n",
    "    prob_2 = comp_2_freq / unigram_frequencies.total()\n",
    "\n",
    "    assoc_f = compute_functions.get_kld(np.array([prob_2_1, 1 - prob_2_1]),\n",
    "        np.array([prob_2, 1 - prob_2]))\n",
    "    assoc_b = compute_functions.get_kld(np.array([prob_1_2, 1 - prob_1_2]),\n",
    "        np.array([prob_1, 1 - prob_1]))\n",
    "    return assoc_f, assoc_b\n",
    "\n",
    "def get_entropy_dif(ngram_1_freqs, ngram_2):\n",
    "    \"\"\"\n",
    "    Function to obtain the difference in the entropy of a \n",
    "    slot in the ngram and the entropy if the target component\n",
    "    was eliminated from the distribution.\n",
    "    :param ngram_1_freqs: Frequency distribution (nltk.FreqDist)\n",
    "        of the successors of the target slot.\n",
    "    :param ngram_2: A string specifying the occurrying component\n",
    "        of the ngram to be eliminated from the frequency distribution.\n",
    "    :returns: The difference, as counterfactual - actual. Scalar.\n",
    "    \"\"\"\n",
    "    slot_dist = np.array(list(ngram_1_freqs.values()))\n",
    "    entropy = compute_functions.get_entropy(slot_dist)\n",
    "    freqs_cf = ngram_1_freqs.copy()\n",
    "    _ = freqs_cf.pop(ngram_2)\n",
    "    freqs_cf = np.array(list(freqs_cf.values()))\n",
    "    entropy_cf = compute_functions.get_entropy(freqs_cf)\n",
    "    h_diff = entropy_cf - entropy\n",
    "    return h_diff\n",
    "    \n",
    "def get_ngram_scores(ngram, corpus, verbose=False):\n",
    "    \"\"\"\n",
    "    Function for computing the MWU measures for a target ngram. \n",
    "    :param ngram: A string with the ngram to be analyzed.\n",
    "    :returns: A dictionary with all MWU measures obtained:\n",
    "        Token frequency, dispersion, type frequency for each slot,\n",
    "        entropy difference for each slot, both directions of \n",
    "        association.\n",
    "    \"\"\"\n",
    "    comps = ngram.split(' ')\n",
    "    comp_1 = comps[0]\n",
    "    comp_2 = comps[1]\n",
    "    if len(comps) == 2:\n",
    "        this_type = 'bigram'\n",
    "        comp_3 = ''\n",
    "    elif len(comps) == 3:\n",
    "        this_type = 'trigram'\n",
    "        comp_3 = comps[2]\n",
    "    else:\n",
    "        print('Error! ngram length not supported')\n",
    "\n",
    "    fw_dist = corpus.get_fw_distribution(ngram)\n",
    "    bw_dist = corpus.get_bw_distribution(ngram)\n",
    "\n",
    "    if this_type == 'bigram':    \n",
    "        ngram_freq = {this_corpus: freq.get(comp_2, 0) for this_corpus, freq in fw_dist.items()}\n",
    "    if this_type == 'trigram':\n",
    "        ngram_freq = {this_corpus: freq.get(comp_3, 0) for this_corpus, freq in fw_dist.items()}\n",
    "    \n",
    "    # Token frequency\n",
    "    token_freq = sum(ngram_freq.values())\n",
    "    if token_freq == 0:\n",
    "        print(f'<<{\" \".join([comp_1, comp_2, comp_3])}>> is not in the corpus')\n",
    "        return None\n",
    "    # Dispersion\n",
    "    corpus_proportions = corpus.corpus_proportions\n",
    "    dispersion = get_dispersion(ngram_freq, token_freq, corpus_proportions)\n",
    "    \n",
    "    # Total frequencies\n",
    "    fw_dist = sum(fw_dist.values(), Counter())\n",
    "    bw_dist = sum(bw_dist.values(), Counter())\n",
    "\n",
    "    # Type frequencies\n",
    "    typef_1 = len(bw_dist)\n",
    "    typef_2 = len(fw_dist)\n",
    "\n",
    "    # Entropy\n",
    "    if this_type == 'bigram':\n",
    "        slot1_diff = get_entropy_dif(bw_dist, comp_1)\n",
    "        slot2_diff = get_entropy_dif(fw_dist, comp_2)\n",
    "    elif this_type == 'trigram':\n",
    "        slot1_diff = get_entropy_dif(bw_dist, (comp_1, comp_2))\n",
    "        slot2_diff = get_entropy_dif(fw_dist, comp_3)\n",
    "\n",
    "    unigram_dict = corpus.total_unigrams\n",
    "    n_trigrams = corpus.n_trigrams\n",
    "    # Association\n",
    "    if this_type == 'bigram':\n",
    "        part_1 = comp_1\n",
    "        part_2 = comp_2\n",
    "    elif this_type == 'trigram':\n",
    "        part_1 = (comp_1, comp_2)\n",
    "        part_2 = comp_3\n",
    "    assoc_f, assoc_b = get_association(part_1, part_2, token_freq, unigram_dict, fw_dist, n_trigrams) \n",
    "\n",
    "\n",
    "    if this_type == 'bigram':\n",
    "        return {\n",
    "            'ngram': (comp_1, comp_2), \n",
    "            'first': comp_1,\n",
    "            'second': comp_2,\n",
    "            'token_freq': token_freq,\n",
    "            'dispersion': dispersion,\n",
    "            'type_1': typef_1,\n",
    "            'type_2': typef_2,\n",
    "            'entropy_1': slot1_diff,\n",
    "            'entropy_2': slot2_diff,\n",
    "            'assoc_f': assoc_f,\n",
    "            'assoc_b': assoc_b\n",
    "            }\n",
    "    elif this_type == 'trigram':\n",
    "        return {\n",
    "            'ngram': (comp_1, comp_2, comp_3), \n",
    "            'first': ' '.join([comp_1, comp_2]),\n",
    "            'second': comp_3,\n",
    "            'token_freq': token_freq,\n",
    "            'dispersion': dispersion,\n",
    "            'type_1': typef_1,\n",
    "            'type_2': typef_2,\n",
    "            'entropy_1': slot1_diff,\n",
    "            'entropy_2': slot2_diff,\n",
    "            'assoc_f': assoc_f,\n",
    "            'assoc_b': assoc_b\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8928 lines processed\n",
      "16500 lines processed\n",
      "25755 lines processed\n",
      "34338 lines processed\n",
      "42803 lines processed\n",
      "52217 lines processed\n",
      "61689 lines processed\n",
      "71326 lines processed\n",
      "80991 lines processed\n",
      "90366 lines processed\n",
      "98040 lines processed\n",
      "105999 lines processed\n",
      "114462 lines processed\n",
      "123188 lines processed\n",
      "130358 lines processed\n",
      "137924 lines processed\n",
      "147611 lines processed\n",
      "157164 lines processed\n",
      "168734 lines processed\n",
      "178303 lines processed\n",
      "188473 lines processed\n",
      "196901 lines processed\n",
      "206573 lines processed\n",
      "220860 lines processed\n",
      "237194 lines processed\n",
      "250060 lines processed\n",
      "261943 lines processed\n",
      "270684 lines processed\n",
      "278913 lines processed\n",
      "287810 lines processed\n",
      "296127 lines processed\n",
      "301046 lines processed\n"
     ]
    }
   ],
   "source": [
    "this_corpus = mwu_measures.processing_corpus.process_corpus(corpus_name='bnc', corpus_dir='small_corpus.txt', verbose=True, chunk_size=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_token_trigram</th>\n",
       "      <th>max_type1_trigram</th>\n",
       "      <th>max_type2_trigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1778.0</td>\n",
       "      <td>115430</td>\n",
       "      <td>10735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_token_trigram  max_type1_trigram  max_type2_trigram\n",
       "0             1778.0             115430              10735"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trigram mins and max. something seems wrong? Nope, it's correct\n",
    "this_corpus.corpus_conn.execute(\"\"\"\n",
    "    WITH trigram_totals AS (\n",
    "        SELECT ug_1, ug_2, ug_3, SUM(freq) as freq\n",
    "        FROM trigram_db\n",
    "        GROUP BY ug_1, ug_2, ug_3\n",
    "    ),\n",
    "    token_frequency AS (\n",
    "        SELECT \n",
    "            max(freq) AS max_token_trigram\n",
    "        FROM trigram_totals\n",
    "    ),\n",
    "    type_1 AS (\n",
    "        SELECT max(typef_1) as max_type1_trigram\n",
    "        FROM (\n",
    "            SELECT ug_3, count( distinct concat(ug_1, ug_2) ) AS typef_1\n",
    "            FROM trigram_totals\n",
    "            GROUP BY ug_3\n",
    "        )\n",
    "    ),\n",
    "    type_2 AS (\n",
    "    SELECT max(typef_2) AS max_type2_trigram\n",
    "    FROM (\n",
    "        SELECT ug_1, ug_2, count( distinct ug_3 ) AS typef_2\n",
    "        FROM trigram_totals\n",
    "        GROUP BY ug_1, ug_2\n",
    "        )\n",
    "    )\n",
    "    SELECT token_frequency.max_token_trigram, type_1.max_type1_trigram, type_2.max_type2_trigram\n",
    "    FROM token_frequency, type_1, type_2\n",
    "\"\"\").fetch_df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_token_bigram</th>\n",
       "      <th>max_type1_bigram</th>\n",
       "      <th>max_type2_bigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37846.0</td>\n",
       "      <td>27844</td>\n",
       "      <td>33770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_token_bigram  max_type1_bigram  max_type2_bigram\n",
       "0           37846.0             27844             33770"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram max token and types\n",
    "this_corpus.corpus_conn.execute(\"\"\"\n",
    "    WITH bigram_totals AS (\n",
    "        SELECT ug_1, ug_2, SUM(freq) as freq\n",
    "        FROM trigram_db\n",
    "        GROUP BY ug_1, ug_2\n",
    "        ),\n",
    "    token_frequency AS (\n",
    "        SELECT \n",
    "            max(freq) AS max_token_bigram\n",
    "        FROM bigram_totals\n",
    "        ),\n",
    "    type_1 AS (\n",
    "        SELECT max(typef_1) AS max_type1_bigram\n",
    "        FROM (\n",
    "            SELECT ug_2, count( distinct ug_1 ) AS typef_1\n",
    "            FROM bigram_totals\n",
    "            GROUP BY ug_2\n",
    "            )\n",
    "        ),\n",
    "    type_2 AS (\n",
    "        SELECT max(typef_2) AS max_type2_bigram\n",
    "        FROM (\n",
    "            SELECT ug_1, count( distinct ug_2 ) AS typef_2\n",
    "            FROM bigram_totals\n",
    "            GROUP BY ug_1\n",
    "            )\n",
    "        )\n",
    "    SELECT token_frequency.max_token_bigram, type_1.max_type1_bigram, type_2.max_type2_bigram\n",
    "    FROM token_frequency, type_1, type_2\n",
    "\"\"\").fetch_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mwu_measures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
