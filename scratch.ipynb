{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwu_measures\n",
    "import pandas as pd\n",
    "import mwu_measures.preprocessing_corpus\n",
    "import mwu_measures.processing_corpus\n",
    "from mwu_measures.corpus_helper import Fetcher\n",
    "from mwu_measures.corpus import Corpus\n",
    "import duckdb\n",
    "from rich.progress import Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mwu_examples = pd.read_csv('MultiwordExpression_Concreteness_Ratings.csv')\n",
    "mwu_examples['length'] = mwu_examples['Expression'].apply(lambda x: len(x.split()))\n",
    "mwu_examples = mwu_examples.loc[(mwu_examples['length'] == 2) | (mwu_examples['length'] == 3)]\n",
    "mwu_examples['Expression'] = mwu_examples['Expression'].apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB doesn't exist. Allocate before calculations.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mwu_measures/corpora/coca/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pablo/Documents/GitHub/MWU_measures/scratch.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pablo/Documents/GitHub/MWU_measures/scratch.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mwu_measures\u001b[39m.\u001b[39mprocessing_corpus\u001b[39m.\u001b[39mmake_processed_corpus(\u001b[39m'\u001b[39m\u001b[39mcoca_fourgrams\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmwu_measures/corpora/coca/\u001b[39m\u001b[39m'\u001b[39m, chunk_size\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pablo/Documents/GitHub/MWU_measures/scratch.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_corpus \u001b[39m=\u001b[39m Corpus(\u001b[39m\"\u001b[39m\u001b[39mcoca_fourgrams\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pablo/Documents/GitHub/MWU_measures/scratch.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m helper \u001b[39m=\u001b[39m Fetcher(test_corpus)\n",
      "File \u001b[0;32m~/Documents/GitHub/MWU_measures/mwu_measures/processing_corpus.py:73\u001b[0m, in \u001b[0;36mmake_processed_corpus\u001b[0;34m(corpus_name, corpus_dir, verbose, test_corpus, chunk_size, threshold)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39melif\u001b[39;00m (corpus_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcoca\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m corpus_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcoca_sample\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m corpus_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcoca_fourgrams\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m corpus_dir:\n\u001b[1;32m     72\u001b[0m     this_corpus \u001b[39m=\u001b[39m Corpus(corpus_name)\n\u001b[0;32m---> 73\u001b[0m     coca_texts \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(os\u001b[39m.\u001b[39mlistdir(corpus_dir))\n\u001b[1;32m     74\u001b[0m     coca_cats \u001b[39m=\u001b[39m [re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_.+_\u001b[39m\u001b[39m'\u001b[39m, text_name, re\u001b[39m.\u001b[39mIGNORECASE)\u001b[39m.\u001b[39mgroup(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m text_name \u001b[39min\u001b[39;00m coca_texts]\n\u001b[1;32m     75\u001b[0m     coca_cats \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(coca_cats))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mwu_measures/corpora/coca/'"
     ]
    }
   ],
   "source": [
    "mwu_measures.processing_corpus.make_processed_corpus('coca_fourgrams', 'mwu_measures/corpora/coca_texts/', chunk_size=30, verbose=True)\n",
    "test_corpus = Corpus(\"coca_fourgrams\")\n",
    "helper = Fetcher(test_corpus)\n",
    "\n",
    "# wrapper = lp(mwu_measures.processing_corpus.get_processed_corpus)\n",
    "# this_corpus = mwu_measures.processing_corpus.get_processed_corpus('bnc', 'small_corpus.txt', chunk_size=10000000, verbose=False)\n",
    "# this_corpus = mwu_measures.processing_corpus.make_processed_corpus(test_corpus=True, threshold=0)\n",
    "# lp.print_stats()\n",
    "# ngram_selection = [ngram.split() for ngram in ngram_selection]\n",
    "# ngram_chunks = np.array_split(ngram_selection, 100)\n",
    "# helper = Fetcher(this_corpus)\n",
    "# 340 seconds for all _acad_. Not bad, not the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus.df(\"SELECT * FROM corpus_proportions LIMIT 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Hello! Come in and eat boiled potatoes they are one hundred percent good.\\nI remember when we met in the year 2000'\n",
    "sentence = mwu_measures.process_text(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = helper.get_score_batch(sentence, from_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['raw'][x['raw'].ngram_length == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures.processing_corpus.make_processed_corpus('test', test_corpus=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = Corpus(\"test\")\n",
    "helper = Fetcher(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [['b', 'd'], ['c', 'b'], ['a', 'c']]\n",
    "test_corpus.create_query(bigrams, 'ug_1', 'ug_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus.get_ngram_scores('ug_1', 'ug_2', 2)['raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_corpus = Corpus(\"coca\")\n",
    "helper = Fetcher(this_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = mwu_examples['Expression'].tolist()\n",
    "# ngrams = [[ngram[0] + ' ' + ngram[1], ngram[2]] if len(ngram) == 3 else ngram for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = helper.get_score_batch(ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x[0][~x[0]['ngram_length'].isna()].drop_duplicates().sort_values(by=['mwu_score'], ascending=False).iloc[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data_1 = pd.read_csv('test.csv', usecols=['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'])\n",
    "human_data_2 = pd.read_csv('train.csv', usecols=['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'])\n",
    "human_data_3 = pd.read_csv('valid.csv', usecols=['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'])\n",
    "human_data = pd.concat([human_data_1, human_data_2, human_data_3]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data['utterance'] = human_data['utterance'].str.replace('_comma_', ' , ')\n",
    "human_data['prompt'] = human_data['prompt'].str.replace('_comma_', ' , ')\n",
    "human_data['utterance'] = human_data['utterance'].str.replace('n t ', \"n't \", regex=False)\n",
    "human_data['prompt'] = human_data['prompt'].str.replace('n t ', \"n't \", regex=False)\n",
    "human_data['utterance'] = human_data['utterance'].str.replace(r'n t[$\\.]', \"n't\", regex=True)\n",
    "human_data['prompt'] = human_data['prompt'].str.replace(r'n t[$\\.]', \"n't\", regex=True)\n",
    "# human_data['prompt'] = human_data['prompt'].str.replace('wasnt', \"wasn't\", regex=True)\n",
    "# human_data['utterance'] = human_data['utterance'].str.replace('wasnt', \"wasn't\", regex=True)\n",
    "\n",
    "human_utterances = human_data.utterance.apply(mwu_measures.process_text)\n",
    "human_prompts = human_data.prompt.apply(mwu_measures.process_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = pd.concat([human_utterances, human_prompts])\n",
    "human_text = human_text.explode()\n",
    "human_text = human_text.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = human_text.drop_duplicates().dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batches = [human_text.iloc[i*50000:(i+1)*50000].copy() for i in range(int(len(human_text) / 50000) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batches = [batch.to_list() for batch in text_batches]\n",
    "# TODO: does wasn t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weights = {'token_freq': 1/8, 'dispersion': 1/4, 'type_1': 1/16, 'type_2': 1/8, 'entropy_1': 1/16, 'entropy_2': 1/8, 'fw_assoc': 1/8, 'bw_assoc': 1/16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text_mwu = []\n",
    "i = 0\n",
    "for batch in text_batches:\n",
    "    print((i / len(text_batches)))\n",
    "    human_text_mwu.append(helper.get_score_batch(batch, weights=test_weights, from_text=True))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text_mwu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((result[0] for result in human_text_mwu)).sort_values(by = 'ngram').to_csv('human_mwu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text_mwu.drop_duplicates().sort_values(by=['mwu_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LO HICE!!!\n",
    "this_corpus.create_query(ngrams, 'big_1', 'ug_3')\n",
    "this_corpus.get_ngram_scores('big_1', 'ug_3', 3, [-0.1, 0.1])['normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_corpus.df(\"SELECT * FROM trigram_db WHERE big_1 = HASH('read up') AND ug_3 = HASH('about')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper(\"SELECT DISTINCT * FROM entropy_diffs\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy broken for trigrams again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = helper.get_score_batch(mwu_examples['Expression']).sort_values(by='mwu_score')\n",
    "x[x['ngram_length'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper(\"SELECT * FROM raw_measures\", df = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = ['come in handy', 'family friend', 'database management system', 'one hundred percent', 'line of control', 'like a', 'boiled potatoes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = helper\n",
    "normalized=True\n",
    "# self.get_score_batch(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.create_scores(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.get_score_batch(sentence)\n",
    "# TODO: why are they repeated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_corpus.df(\"VACUUM ANALYZE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rewrite documentation and merge onto main. This is basically ready.\n",
    "# TODO: simplify process_text flowline. default for helper?? process > create > return? > clean?\n",
    "# TODO: Brown as default corpus.\n",
    "# > this for after cogsci\n",
    "# TODO: 4-grams. Should need minimal modification. Challenge might be RAM. Consider implementing an option to work from disk with duckdb?\n",
    "# > This for after cogsci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data = pd.read_csv('2GPTEmpathicDialoguesDataset (1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_utterances = gpt_data.processed.apply(mwu_measures.process_text)\n",
    "gpt_prompts = gpt_data.prompt.apply(mwu_measures.process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_text = pd.concat([gpt_utterances, gpt_prompts])\n",
    "gpt_text = gpt_text.explode()\n",
    "gpt_text = gpt_text.dropna()\n",
    "gpt_text = gpt_text.drop_duplicates().dropna().reset_index(drop=True)\n",
    "text_batches = [gpt_text.iloc[i*50000:(i+1)*50000].copy() for i in range(int(len(gpt_text) / 50000) + 1)]\n",
    "text_batches = [batch.to_list() for batch in text_batches]\n",
    "nice_weights = {'token_freq': 1/8, 'dispersion': 1/4, 'type_1': 1/16, 'type_2': 1/8, 'entropy_1': 1/16, 'entropy_2': 1/8, 'fw_assoc': 1/8, 'bw_assoc': 1/16}\n",
    "gpt_text_mwu = []\n",
    "i = 0\n",
    "for batch in text_batches:\n",
    "    print(round(i / len(text_batches), 2))\n",
    "    gpt_text_mwu.append(helper.get_score_batch(batch, weights=nice_weights, from_text=True))\n",
    "    i += 1\n",
    "\n",
    "pd.concat((result[0] for result in gpt_text_mwu)).sort_values(by = 'ngram').to_csv('gpt_mwu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data_1 = pd.read_csv('test.csv', usecols=['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'])\n",
    "human_data_2 = pd.read_csv('train.csv', usecols=['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'])\n",
    "human_data_3 = pd.read_csv('valid.csv', usecols=['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'])\n",
    "human_data = pd.concat([human_data_1, human_data_2, human_data_3]).reset_index(drop=True)\n",
    "human_data['utterance'] = human_data['utterance'].str.replace('_comma_', ' , ')\n",
    "human_data['prompt'] = human_data['prompt'].str.replace('_comma_', ' , ')\n",
    "human_data['utterance'] = human_data['utterance'].str.replace('n t ', \"n't \", regex=False)\n",
    "human_data['prompt'] = human_data['prompt'].str.replace('n t ', \"n't \", regex=False)\n",
    "human_data['utterance'] = human_data['utterance'].str.replace(r'n t[$\\.]', \"n't\", regex=True)\n",
    "human_data['prompt'] = human_data['prompt'].str.replace(r'n t[$\\.]', \"n't\", regex=True)\n",
    "# human_data['prompt'] = human_data['prompt'].str.replace('wasnt', \"wasn't\", regex=True)\n",
    "# human_data['utterance'] = human_data['utterance'].str.replace('wasnt', \"wasn't\", regex=True)\n",
    "\n",
    "human_data['utterance'] = human_data.utterance.apply(mwu_measures.process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_scores = pd.read_csv('human_mwu_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_scores = dict(zip(human_scores['ngram'].to_list(), human_scores['mwu_score'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data = human_data.explode('utterance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data = human_data[~human_data['utterance'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data['mwu_score'] = human_data.utterance.apply(lambda x: human_scores[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data = human_data.reset_index(drop=True)\n",
    "human_data['trigram_id'] = human_data.index // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data[['conv_id', 'utterance_idx', 'prompt', 'speaker_idx', 'utterance', 'mwu_score', 'trigram_id']].to_csv('human_trigram_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data = pd.read_csv('2GPTEmpathicDialoguesDataset (1).csv')\n",
    "gpt_data['utterance'] = gpt_data.processed.apply(mwu_measures.process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_scores = pd.read_csv('gpt_mwu_scores.csv')\n",
    "gpt_scores = dict(zip(gpt_scores['ngram'].to_list(), gpt_scores['mwu_score'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data = gpt_data.explode('utterance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data = gpt_data[~gpt_data['utterance'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data['mwu_score'] = gpt_data.utterance.apply(lambda x: gpt_scores[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data = gpt_data.reset_index(drop=True)\n",
    "gpt_data['trigram_id'] = gpt_data.index // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data[['conv_id', 'prompt', 'utterance', 'mwu_score', 'trigram_id']].to_csv('gpt_trigram_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mwu_measures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
