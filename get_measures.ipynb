{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_groupies as npg\n",
    "from numpy.lib import recfunctions\n",
    "import string\n",
    "import re\n",
    "import compute_functions\n",
    "import mwu_functions\n",
    "import preprocess_corpus\n",
    "import process_corpus\n",
    "punctuation = string.punctuation\n",
    "from importlib import reload\n",
    "process_corpus = reload(process_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://journals.openedition.org/lexis/6231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = preprocess_corpus.preprocess_bnc('../mwu_chunkyness/bnc/2.0/bnc_tokenized.txt')\n",
    "# Make into list, easier to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unigrams, all_bigrams = process_corpus.get_ngrams(corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_freqs_pc, uni_freqs = process_corpus.get_frequencies(all_unigrams)\n",
    "bi_freqs_pc, bi_freqs = process_corpus.get_frequencies(all_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_id, bigram_id = process_corpus.get_ids(uni_freqs, bi_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freqs_array = process_corpus.get_bigram_info(bi_freqs_pc, unigram_id, bigram_id, by_corpus=True)\n",
    "bigram_freqs_agg = process_corpus.get_bigram_info(bi_freqs, unigram_id, bigram_id, by_corpus=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_props = process_corpus.get_corpus_props(bigram_freqs_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('!', '!',      0,      0, 'C', 1,        0),\n",
       "       ('!', '!',      0,      0, 'H', 1,        0),\n",
       "       ('!', 'a',      0,  80255, 'J', 1,        1), ...,\n",
       "       ('♭7th', 'd', 635267, 192402, 'C', 1, 12453927),\n",
       "       ('♭7th', 'g', 635267, 262716, 'C', 1, 12453928),\n",
       "       ('♯4/♯11', 'chords', 635268, 165199, 'C', 1, 12453929)],\n",
       "      dtype=[('first', 'O'), ('second', 'O'), ('first_id', '<i4'), ('second_id', '<i4'), ('corpus', 'O'), ('freq', '<i4'), ('bigram_id', '<i4')])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freqs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_1 = '!'\n",
    "comp_2 = \"!\"\n",
    "bigram_freqs_per_corpus = bigram_freqs_array\n",
    "bigram_freqs_total = bigram_freqs_agg\n",
    "unigram_freqs = uni_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freq = bigram_freqs_per_corpus[np.logical_and(bigram_freqs_per_corpus['first'] == comp_1, bigram_freqs_per_corpus['second'] == comp_2)]\n",
    "bigram_props = bigram_freq['freq'] / np.sum(bigram_freq['freq'])\n",
    "bigram_props = zip(bigram_freq['corpus'], bigram_props)\n",
    "bigram_props = pd.DataFrame(bigram_props, columns=['corpus', 'ngram_prop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_props = pd.merge(corpus_props, bigram_props, on='corpus', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/pablo/Documents/GitHub/MWU_measures/get_measures.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pablo/Documents/GitHub/MWU_measures/get_measures.ipynb#Y304sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ngram_corpus_props \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m\u001b[39m.\u001b[39mfromkeys(corpus_props\u001b[39m.\u001b[39;49mkeys(), \u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pablo/Documents/GitHub/MWU_measures/get_measures.ipynb#Y304sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m corpus, prop \u001b[39min\u001b[39;00m bigram_props:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pablo/Documents/GitHub/MWU_measures/get_measures.ipynb#Y304sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     ngram_corpus_props[corpus] \u001b[39m=\u001b[39m prop\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "ngram_corpus_props = dict.fromkeys(corpus_props.keys(), 0)\n",
    "for corpus, prop in bigram_props:\n",
    "    ngram_corpus_props[corpus] = prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_corpus_props = np.array(list(ngram_corpus_props.items()), dtype = {'names':['corpus', 'ngram_prop'], 'formats':[np.dtypes.StrDType, 'f']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_props = np.array(list(corpus_props.items()), dtype = {'names': ['corpus', 'corpus_prop'], 'formats': [np.dtypes.StrDType, 'f']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('D', 0.00084524), ('K', 0.1146091 ), ('C', 0.17681658),\n",
       "       ('F', 0.09082857), ('H', 0.19949414), ('G', 0.07913015),\n",
       "       ('B', 0.06791935), ('E', 0.06830774), ('J', 0.0592792 ),\n",
       "       ('A', 0.14276995)], dtype=[('corpus', 'O'), ('corpus_prop', '<f4')])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_corpus_props[bigram_freq['corpus']] = bigram_props # This handles dispersion with bigrams with 0 occurrence in 1 or more corpora\n",
    "dispersion = compute_functions.get_KLD(ngram_corpus_props, corpus_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('hello', \"i'm\", 293638, 307256, 'A',  1, 5036258),\n",
       "       ('hello', \"i'm\", 293638, 307256, 'C',  2, 5036258),\n",
       "       ('hello', \"i'm\", 293638, 307256, 'E',  1, 5036258),\n",
       "       ('hello', \"i'm\", 293638, 307256, 'F',  4, 5036258),\n",
       "       ('hello', \"i'm\", 293638, 307256, 'G',  2, 5036258),\n",
       "       ('hello', \"i'm\", 293638, 307256, 'H',  5, 5036258),\n",
       "       ('hello', \"i'm\", 293638, 307256, 'J',  3, 5036258),\n",
       "       ('hello', \"i'm\", 293638, 307256, 'K', 16, 5036258)],\n",
       "      dtype=[('first', 'O'), ('second', 'O'), ('first_id', '<i4'), ('second_id', '<i4'), ('corpus', 'O'), ('freq', '<i4'), ('bigram_id', '<i4')])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09090909, 0.09090909, 0.45454545, 0.09090909, 0.09090909,\n",
       "       0.09090909, 0.09090909])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get normalization scores: max and min token freq and type freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First sum across corpora\n",
    "type_1 = npg.aggregate(bigram_freqs_agg['first_id'], 1)\n",
    "type_1 = type_1[bigram_freqs_agg['first_id']]\n",
    "type_2 = npg.aggregate(bigram_freqs_agg['second_id'], 1) \n",
    "type_2 = type_2[bigram_freqs_agg['second_id']]\n",
    "log_type_1 = np.log(type_1)\n",
    "log_type_2 = np.log(type_2)\n",
    "max_type_1 = np.max(log_type_1)\n",
    "min_type_1 = np.min(log_type_1)\n",
    "max_type_2 = np.max(log_type_2)\n",
    "min_type_2 = np.min(log_type_2)\n",
    "\n",
    "min_token = np.min(np.log(bigram_freqs_agg['freq']))\n",
    "max_token = np.max(np.log(bigram_freqs_agg['freq']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram_freq = bigram_freqs_array[np.logical_and(bigram_freqs_array['first'] == test_1, bigram_freqs_array['second'] == test_2)]\n",
    "# bigram_props = bigram_freq['freq'] / np.sum(bigram_freq['freq'])\n",
    "# ngram_corpus_props = np.zeros(corpus_sizes.shape[0])\n",
    "# ngram_corpus_props[bigram_freq['corpus']] = bigram_props # This handles dispersion with bigrams with 0 occurrence in 1 or more corpora\n",
    "# token_freq = np.sum(bigram_freq['freq']) # Remember to log later. Keep it non-log to use in calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_info = np.log2((ngram_corpus_props / corpus_props))\n",
    "# x_info[x_info == -np.inf] = 0\n",
    "# KLD_ngram_corpus = np.sum(x_info)\n",
    "# dispersion = 1 - (np.power(np.e, KLD_ngram_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need aggregate frequencies for the rest\n",
    "# freqs_ngram_1 = bigram_freqs_agg[bigram_freqs_agg['first'] == test_1]\n",
    "# freqs_ngram_2 = bigram_freqs_agg[bigram_freqs_agg['second'] == test_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typef_1 = freqs_ngram_1.shape[0]\n",
    "# typef_2 = freqs_ngram_2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 3])\n",
    "y = np.array([3, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(x * np.log(x / y)))\n",
    "print(np.inner(x, np.log(x / y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized difference of entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqs_ngram_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqs_1 = freqs_ngram_1['freq']\n",
    "# freqs_2 = freqs_ngram_2['freq']\n",
    "# dist_1 = freqs_1 / np.sum(freqs_1)\n",
    "# dist_2 = freqs_2 / np.sum(freqs_2)\n",
    "# dist_1_diff = np.delete(freqs_1, np.where(freqs_ngram_1['second'] == test_2))\n",
    "# dist_1_diff = dist_1_diff / np.sum(dist_1_diff)\n",
    "# dist_2_diff = np.delete(freqs_2, np.where(freqs_ngram_2['first'] == test_1))\n",
    "# dist_2_diff = dist_2_diff / np.sum(dist_2_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_1 = (-(np.sum(np.log2(dist_1) * dist_1))) / np.log2(dist_1.shape[0])\n",
    "# h_1_diff = (-(np.sum(np.log2(dist_1_diff) * dist_1_diff))) / np.log2(dist_1_diff.shape[0])\n",
    "# h_2 = (-(np.sum(np.log2(dist_2) * dist_2))) / np.log2(dist_2.shape[0])\n",
    "# h_2_diff = (-(np.sum(np.log2(dist_2_diff) * dist_2_diff))) / np.log2(dist_2_diff.shape[0])\n",
    "# h_ngram_1 = h_1_diff - h_1\n",
    "# h_ngram_2 = h_2_diff - h_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 to 2: (Prob 1 and 2 vs prob 1 and other) vs (prob 2 vs prob not 2)\n",
    "\n",
    "2 to 1: (Prob 1 and 2 vs prob other and 2) vs (prob 1 vs prob not 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_1_2 = token_freq / np.sum(freqs_1)\n",
    "# prob_1_o = 1 - prob_1_2\n",
    "# prob_2_1 = token_freq / np.sum(freqs_2)\n",
    "# prob_o_2 = 1 - prob_2_1 \n",
    "\n",
    "# prob_1 = np.sum(freqs_1) / np.sum(bigram_freqs_agg['freq'])\n",
    "# prob_no_1 = 1 - prob_1\n",
    "# prob_2 = np.sum(freqs_2) / np.sum(bigram_freqs_agg['freq'])\n",
    "# prob_no_2 = 1 - prob_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLD_1_2 = prob_1_2 * np.log2(prob_1_2 / prob_2) + prob_1_o * np.log2(prob_1_o / prob_no_2)\n",
    "# KLD_2_1 = prob_2_1 * np.log2(prob_2_1 / prob_1) + prob_o_2 * np.log2(prob_o_2 / prob_no_1)\n",
    "# assoc_f = 1 - np.power(np.e, -KLD_1_2)\n",
    "# assoc_b = 1 - np.power(np.e, -KLD_2_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make result dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mwu_scores = {'ngram': (test_1, test_2), 'first': test_1, 'second': test_2, 'token_freq': np.log(token_freq), 'dispersion': 1 - dispersion, 'type_1': np.log(typef_1), 'type_2': np.log(typef_2), 'entropy_1': h_ngram_1, 'entropy_2': h_ngram_2, 'assoc_f': assoc_f, 'assoc_b': assoc_b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mwu_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_entropy(array):\n",
    "#     if len(array) == 0:\n",
    "#         return 1.0\n",
    "#     if len(array) == 1:\n",
    "#         return 0.0\n",
    "#     else:\n",
    "#         probs = array / np.sum(array)\n",
    "#         logs = np.log2(probs)\n",
    "#         return -np.nansum(probs * logs) / np.log2(array.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freqs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bigram_scores(comp_1, comp_2, bigram_freqs_per_corpus = bigram_freqs_array, corpus_props = corpus_props, bigram_freqs_total = bigram_freqs_agg):\n",
    "#     # By default it looks for variables called bigram_freqs_array, corpus_props, and bigram_freqs_total.\n",
    "#     bigram_freq = bigram_freqs_per_corpus[np.logical_and(bigram_freqs_array['first'] == comp_1, bigram_freqs_per_corpus['second'] == comp_2)]\n",
    "#     bigram_props = bigram_freq['freq'] / np.sum(bigram_freq['freq'])\n",
    "    \n",
    "#     # Token frequency\n",
    "#     token_freq = np.sum(bigram_freq['freq'])\n",
    "\n",
    "#     # Dispersion\n",
    "#     ngram_corpus_props = np.zeros(corpus_sizes.shape[0])\n",
    "#     ngram_corpus_props[bigram_freq['corpus']] = bigram_props # This handles dispersion with bigrams with 0 occurrence in 1 or more corpora\n",
    "#     dispersion = compute_functions.get_KLD(ngram_corpus_props, corpus_props)\n",
    "    \n",
    "#     ## Need aggregate frequencies for the rest\n",
    "#     freqs_ngram_1 = bigram_freqs_agg[bigram_freqs_total['first'] == comp_1]\n",
    "#     freqs_ngram_2 = bigram_freqs_agg[bigram_freqs_total['second'] == comp_2]\n",
    "\n",
    "#     # Type frequencies    \n",
    "#     typef_1 = freqs_ngram_2.shape[0]\n",
    "#     typef_2 = freqs_ngram_1.shape[0] # Type frequency of slot 1 is the number of rows that have slot 1 as first\n",
    "\n",
    "#     # Entropies\n",
    "\n",
    "#     slot1_diff = compute_functions.get_entropy_dif(freqs_ngram_2, comp_1, 'first')\n",
    "#     slot2_diff = compute_functions.get_entropy_dif(freqs_ngram_1, comp_2, 'second')\n",
    "\n",
    "#     # Associations\n",
    "#     # joint probability is conditioned on the unigram frequency of the 1st one\n",
    "#     prob_1_2 = token_freq / uni_freqs[(comp_1,)]\n",
    "#     prob_2_1 = token_freq / uni_freqs[(comp_2,)]\n",
    "#     prob_1 = uni_freqs.freq((comp_1,))\n",
    "#     prob_2 = uni_freqs.freq((comp_2,))\n",
    "#     assoc_f = compute_functions.get_KLD(np.array([prob_1_2, 1 - prob_1_2]), np.array([prob_2, 1 - prob_2]))\n",
    "#     assoc_b = compute_functions.get_KLD(np.array([prob_2_1, 1 - prob_2_1]), np.array([prob_1, 1 - prob_1]))\n",
    "\n",
    "#     return {'ngram': (comp_1, comp_2), 'first': comp_1, 'second': comp_2, 'token_freq': token_freq, 'dispersion': dispersion, 'type_1': typef_1, 'type_2': typef_2, 'entropy_1': slot1_diff, 'entropy_2': slot2_diff, 'assoc_f': assoc_f, 'assoc_b': assoc_b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = mwu_functions.get_bigram_scores('c', 'b', bigram_freqs_array, corpus_props, bigram_freqs_agg, uni_freqs)\n",
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = get_bigram_scores('c', 'b')\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mwu_scores(ngrams):\n",
    "    # ngrams is a list of ngrams separated by spaces \n",
    "    all_scores = []\n",
    "    i = 0\n",
    "    for ngram in ngrams:\n",
    "        print(ngram)\n",
    "        print(i)\n",
    "        comps = ngram.split(' ')\n",
    "        if len(comps) == 2:\n",
    "            bigram_scores = get_bigram_scores(comps[0], comps[1])\n",
    "            all_scores.append(bigram_scores)\n",
    "        else:\n",
    "            bigram_1_score = get_bigram_scores(comps[0], comps[1])\n",
    "            bigram_2_score = get_bigram_scores(comps[1], comps[2])\n",
    "            all_scores.append(bigram_1_score)\n",
    "            all_scores.append(bigram_2_score)\n",
    "        i += 1\n",
    "    return pd.DataFrame(all_scores)\n",
    "\n",
    "# TODO: something's making MWU scores very uniform (~0.3. Whatsup with that?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_mwu_scores(['b d', 'c b', 'a c', 'd c b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_mwu_scores(['a c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with whole corpus to validate program\n",
    "# bigrams_bnc = pd.read_csv('scores/bigram_scores_bnc.csv')\n",
    "filter_bigs = list(filter(lambda x: x[1]>3, bi_freqs.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "bigram_sample = random.sample(filter_bigs, 10000)\n",
    "bigram_sample = [bigram[0][0] + ' ' + bigram[0][1] for bigram in bigram_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_score_sample = get_mwu_scores(bigram_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams_bnc = bigrams_bnc[bigrams_bnc['token_freq'] > 0.001]\n",
    "# sampled_bigrams = bigrams_bnc.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freqs_agg[bigram_freqs_agg['second'] == 'sgsa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_score_sample[mwu_score_sample.entropy_1 < -0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "counts, bins = np.histogram(mwu_score_sample['entropy_1'], bins=100)\n",
    "plt.stairs(counts, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_scores_filter = mwu_score_sample.loc[(mwu_score_sample['type_1'] > 3) & (mwu_score_sample['type_2'] > 3)]\n",
    "mwu_scores_filter = mwu_scores_filter.astype({'token_freq': 'float', 'type_1': 'float', 'type_2': 'float'})\n",
    "mwu_scores_filter.loc[mwu_scores_filter['entropy_1'] > 0.05, 'entropy_1'] = 0.05\n",
    "mwu_scores_filter.loc[mwu_scores_filter['entropy_2'] > 0.05, 'entropy_2'] = 0.05\n",
    "# mwu_scores_filter.loc[:, 'entropy_1'] = np.cbrt(mwu_scores_filter['entropy_1'])\n",
    "# mwu_scores_filter.loc[:, 'entropy_2'] = np.cbrt(mwu_scores_filter['entropy_2'])\n",
    "mwu_scores_filter.loc[:, 'entropy_1'] = (mwu_scores_filter['entropy_1'] - mwu_scores_filter['entropy_1'].min()) / (mwu_scores_filter['entropy_1'].max() - mwu_scores_filter['entropy_1'].min())\n",
    "mwu_scores_filter.loc[:, 'entropy_2'] = (mwu_scores_filter['entropy_2'] - mwu_scores_filter['entropy_2'].min()) / (mwu_scores_filter['entropy_2'].max() - mwu_scores_filter['entropy_2'].min())\n",
    "mwu_scores_filter.loc[:, 'token_freq'] = np.log(mwu_scores_filter['token_freq'])\n",
    "mwu_scores_filter.loc[:, 'token_freq'] = (mwu_scores_filter['token_freq'] - mwu_scores_filter['token_freq'].min()) / (mwu_scores_filter['token_freq'].max() - mwu_scores_filter['token_freq'].min())\n",
    "mwu_scores_filter.loc[:, 'type_1'] = np.log(mwu_scores_filter['type_1'])\n",
    "mwu_scores_filter.loc[:, 'type_2'] = np.log(mwu_scores_filter['type_2'])\n",
    "mwu_scores_filter.loc[:, 'type_1'] = (mwu_scores_filter['type_1'] - mwu_scores_filter['type_1'].min()) / (mwu_scores_filter['type_1'].max() - mwu_scores_filter['type_1'].min())\n",
    "mwu_scores_filter.loc[:, 'type_2'] = (mwu_scores_filter['type_2'] - mwu_scores_filter['type_2'].min()) / (mwu_scores_filter['type_2'].max() - mwu_scores_filter['type_2'].min())\n",
    "mwu_scores_filter.loc[:, 'type_1'] = 1 - mwu_scores_filter['type_1']\n",
    "mwu_scores_filter.loc[:, 'type_2'] = 1 - mwu_scores_filter['type_2']\n",
    "\n",
    "mwu_scores_filter.loc[:, 'dispersion'] = 1 - mwu_scores_filter.loc[:, 'dispersion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_scores_weighted = mwu_scores_filter.copy()\n",
    "mwu_scores_weighted.token_freq = mwu_scores_weighted.token_freq * 0.25\n",
    "mwu_scores_weighted.dispersion = mwu_scores_weighted.token_freq * 0.25\n",
    "mwu_scores_weighted.type_1 = mwu_scores_weighted.type_1 * 0.25/5\n",
    "mwu_scores_weighted.type_2 = mwu_scores_weighted.type_2 * 0.25/5\n",
    "mwu_scores_weighted.entropy_1 = mwu_scores_weighted.entropy_1 * 0.25/5\n",
    "mwu_scores_weighted.entropy_2 = mwu_scores_weighted.entropy_2 * 0.25/5\n",
    "mwu_scores_weighted.assoc_f = mwu_scores_weighted.assoc_f * 0.25\n",
    "mwu_scores_weighted.assoc_b = mwu_scores_weighted.assoc_b * 0.25/5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_scores_weighted['mwu_score'] = mwu_scores_weighted.sum(numeric_only=True, axis=1)\n",
    "mwu_scores_weighted.sort_values(by=['mwu_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_scores_filter['mwu_score'] = mwu_scores_filter.mean(numeric_only=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_scores_filter.sort_values(by='mwu_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "counts, bins = np.histogram(mwu_scores_filter['entropy_1'], bins=100)\n",
    "plt.stairs(counts, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical rule of thumb: difference in entropy ranges between -0.01 and 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins = np.histogram(np.cbrt(mwu_transformed['entropy_2']), bins=100)\n",
    "plt.stairs(counts, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_score_sample.loc[mwu_score_sample['entropy_1'] > 0.01]\n",
    "mwu_scores_filter = mwu_scores_filter[(mwu_scores_filter['entropy_2'] <= 0.01) & (mwu_scores_filter['entropy_2'] > -0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_scores_filter['h1_normalized'] = (np.cbrt(mwu_scores_filter['entropy_1']) - np.min(np.cbrt(mwu_scores_filter['entropy_1']))) / (np.max(np.cbrt(mwu_scores_filter['entropy_1'])) - np.min(np.cbrt(mwu_scores_filter['entropy_1'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1 / (1 + np.exp(-mwu_scores_filter[mwu_scores_filter['token_freq'] != -np.inf]['entropy_1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins = np.histogram(x, bins=100)\n",
    "plt.stairs(counts, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins = np.histogram(mwu_scores_filter[mwu_scores_filter['token_freq'] != -np.inf]['entropy_1'], bins=100)\n",
    "plt.stairs(counts, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cbrt(mwu_scores_filter['entropy_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arcsinh(mwu_scores_filter[mwu_scores_filter['token_freq'] != -np.inf]['entropy_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins = np.histogram(np.cbrt(mwu_scores_filter[mwu_scores_filter['token_freq'] != -np.inf]['entropy_1']), bins=100)\n",
    "plt.stairs(counts, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(10 + np.sqrt(10) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins = np.histogram(mwu_scores_filter[mwu_scores_filter['token_freq'] != -np.inf]['h1_normalized'], bins=100)\n",
    "plt.stairs(counts, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins = np.histogram(np.arcsinh(mwu_scores_filter[mwu_scores_filter['token_freq'] != -np.inf]['h2_normalized']), bins=100)\n",
    "plt.stairs(counts, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_scores_filter.sort_values(by=['h2_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_mwu_scores_trigram(trigrams):\n",
    "#     # trigrams is a list of trigrams separated by spaces  \n",
    "#     all_scores = []\n",
    "#     for trigram in trigrams:\n",
    "#         comps = bigram.split(' ')\n",
    "#         bigram_1_score = get_bigram_scores(comps[0], comps[1])\n",
    "#         bigram_2_score = get_bigram_scores(comps[1], comps[2])\n",
    "#         all_scores.append(bigram_1_score)\n",
    "#         all_scores.append(bigram_2-score)\n",
    "#     return pd.DataFrame(all_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_data = pd.read_csv('jolsvai_et_al_data.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trigrams = [list(nltk.ngrams(trigram.split('_'), 2)) for trigram in set(this_data['Trigram'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bigrams = list(set([' '.join(bigram) for phrase in all_trigrams for bigram in phrase]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bigrams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = get_mwu_scores(all_bigrams)\n",
    "all_scores.to_csv('scratch.csv')\n",
    "# TODO: all dispersion values pretty high. See what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures = np.zeros(len(uq_bigrams), dtype={'names': ['first', 'second', 'token_freq', 'dispersion', 'type_1', 'type_2', 'entropy_1', 'entropy_2', 'assoc_forw', 'assoc_backw'], 'formats': [np.dtypes.StrDType, np.dtypes.StrDType, 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f']})\n",
    "mwu_measures['first'] = np.array([gram[0] for gram in uq_bigrams], dtype=np.dtypes.StrDType)\n",
    "mwu_measures['second'] = np.array([gram[1] for gram in uq_bigrams], dtype=np.dtypes.StrDType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures_tr = np.zeros(len(uq_trigrams), dtype={'names': ['first', 'second', 'token_freq', 'dispersion', 'type_1', 'type_2', 'entropy_1', 'entropy_2', 'assoc_forw', 'assoc_backw'], 'formats': [np.dtypes.StrDType, np.dtypes.StrDType, 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f']})\n",
    "mwu_measures_tr['first'] = np.array([(gram[0] + ' ' + gram[1]) for gram in uq_trigrams], dtype=np.dtypes.StrDType)\n",
    "mwu_measures_tr['second'] = np.array([gram[2] for gram in uq_trigrams], dtype=np.dtypes.StrDType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_freqs_array['first'] = np.array([str(gram['first']) for gram in all_trigrams_info])\n",
    "trigram_freqs_array['first_id'] = [bigram_id[gram['first']] for gram in all_trigrams_info]\n",
    "trigram_freqs_array['second'] = np.array([gram['second'] for gram in all_trigrams_info])\n",
    "trigram_freqs_array['second_id'] = [unigram_id[gram['second']] for gram in all_trigrams_info]\n",
    "trigram_freqs_array['corpus'] = np.array([gram['corpus'] for gram in all_trigrams_info])\n",
    "trigram_freqs_array['freq'] = np.array([gram['freq'] for gram in all_trigrams_info])\n",
    "trigram_freqs_array['trigram_id'] = np.array([gram['trigram_id'] for gram in all_trigrams_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bigrams_agg = []\n",
    "for gram in bi_freqs.items():\n",
    "    all_bigrams_agg.append({'first': gram[0][0], 'second': gram[0][1], 'freq': gram[1], 'bigram_id': bigram_id[gram[0]]})\n",
    "all_trigrams_agg = []\n",
    "for gram in tri_freqs.items():\n",
    "    all_trigrams_agg.append({'first': (gram[0][0], gram[0][1]), 'second': gram[0][2], 'freq': gram[1], 'trigram_id': trigram_id[gram[0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = {'names':['first', 'second', 'first_id', 'second_id', 'freq', 'bigram_id'], 'formats':[np.dtypes.StrDType, np.dtypes.StrDType, 'i', 'i', 'i', 'i']}\n",
    "bigram_freqs_agg = np.zeros(len(all_bigrams_agg), dtype=dt)\n",
    "bigram_freqs_agg['first'] = np.array([gram['first'] for gram in all_bigrams_agg])\n",
    "bigram_freqs_agg['first_id'] = [unigram_id[gram['first']] for gram in all_bigrams_agg]\n",
    "bigram_freqs_agg['second'] = np.array([gram['second'] for gram in all_bigrams_agg])\n",
    "bigram_freqs_agg['second_id'] = [unigram_id[gram['second']] for gram in all_bigrams_agg]\n",
    "bigram_freqs_agg['freq'] = np.array([gram['freq'] for gram in all_bigrams_agg])\n",
    "bigram_freqs_agg['bigram_id'] = np.array([gram['bigram_id'] for gram in all_bigrams_agg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = {'names':['first', 'second', 'first_id', 'second_id', 'freq', 'trigram_id'], 'formats':[np.dtypes.StrDType, np.dtypes.StrDType, 'i', 'i', 'i', 'i']}\n",
    "trigram_freqs_agg = np.zeros(len(all_trigrams_agg), dtype=dt)\n",
    "trigram_freqs_agg['first'] = np.array([str(gram['first']) for gram in all_trigrams_agg])\n",
    "trigram_freqs_agg['first_id'] = [bigram_id[gram['first']] for gram in all_trigrams_agg]\n",
    "trigram_freqs_agg['second'] = np.array([gram['second'] for gram in all_trigrams_agg])\n",
    "trigram_freqs_agg['second_id'] = [unigram_id[gram['second']] for gram in all_trigrams_agg]\n",
    "trigram_freqs_agg['freq'] = np.array([gram['freq'] for gram in all_trigrams_agg])\n",
    "trigram_freqs_agg['trigram_id'] = np.array([gram['trigram_id'] for gram in all_trigrams_agg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token frequencies, pretty easy, aggregate by bigram frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_frequencies = npg.aggregate(bigram_freqs_array['bigram_id'], bigram_freqs_array['freq'], func='sum')\n",
    "mwu_measures['token_freq'] = bigram_frequencies\n",
    "trigram_frequencies = npg.aggregate(trigram_freqs_array['trigram_id'], trigram_freqs_array['freq'], func='sum')\n",
    "mwu_measures_tr['token_freq'] = trigram_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dispersion will be operationalized here using a normalized version of the KL-divergence. KL-D between occurrence in corpus and corpus total (i.e. distance from chance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total and probs for each corpus\n",
    "corpus_totals = npg.aggregate(bigram_freqs_array['corpus'], bigram_freqs_array['freq'], func='sum')\n",
    "total_words = np.sum(corpus_totals)\n",
    "corpus_totals = corpus_totals[bigram_freqs_array['corpus']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_probs = corpus_totals / total_words\n",
    "corpus_probs = corpus_probs[bigram_freqs_array['corpus']]\n",
    "bigram_frequencies_rep = bigram_frequencies[bigram_freqs_array['bigram_id']]\n",
    "prop_appearing_in_corpus = bigram_freqs_array['freq'] / bigram_frequencies_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KLD = prop_appearing_in_corpus * np.log2(prop_appearing_in_corpus / corpus_probs)\n",
    "KLD = npg.aggregate(bigram_freqs_array['bigram_id'], KLD)\n",
    "KLD_norm = 1 - np.power(np.e, -KLD)\n",
    "\n",
    "mwu_measures['dispersion'] = KLD_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For trigrams\n",
    " \n",
    "corpus_totals = npg.aggregate(trigram_freqs_array['corpus'], trigram_freqs_array['freq'], func='sum')\n",
    "total_words = np.sum(corpus_totals)\n",
    "corpus_totals = corpus_totals[trigram_freqs_array['corpus']]\n",
    "corpus_probs = corpus_totals / total_words\n",
    "corpus_probs = corpus_probs[trigram_freqs_array['corpus']]\n",
    "trigram_frequencies_rep = trigram_frequencies[trigram_freqs_array['trigram_id']]\n",
    "prop_appearing_in_corpus = trigram_freqs_array['freq'] / trigram_frequencies_rep\n",
    "KLD = prop_appearing_in_corpus * np.log2(prop_appearing_in_corpus / corpus_probs)\n",
    "KLD = npg.aggregate(trigram_freqs_array['trigram_id'], KLD)\n",
    "KLD_norm = 1 - np.power(np.e, -KLD)\n",
    "\n",
    "mwu_measures_tr['dispersion'] = KLD_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the type frequencies of each component. From now on, we'll use only the aggregate array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First sum across corpora\n",
    "type_1 = npg.aggregate(bigram_freqs_agg['second_id'], 1)\n",
    "type_2 = npg.aggregate(bigram_freqs_agg['first_id'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures['type_1'] = type_1[bigram_freqs_agg['second_id']]\n",
    "mwu_measures['type_2'] = type_2[bigram_freqs_agg['first_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigrams\n",
    "type_1 = npg.aggregate(trigram_freqs_agg['second_id'], 1)\n",
    "type_2 = npg.aggregate(trigram_freqs_agg['first_id'], 1)\n",
    "mwu_measures_tr['type_1'] = type_1[trigram_freqs_agg['second_id']]\n",
    "mwu_measures_tr['type_2'] = type_2[trigram_freqs_agg['first_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the entropies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's do slot 1. We need the marginal frequency of the word in slot 2 to transform the conditional frequencies of slot 1 into conditional probabilities.\n",
    "slot2_freqs = npg.aggregate(bigram_freqs_agg['second_id'], bigram_freqs_agg['freq'], func='sum')\n",
    "slot2_freqs = slot2_freqs[bigram_freqs_agg['second_id']]\n",
    "slot1_freqs = npg.aggregate(bigram_freqs_agg['first_id'], bigram_freqs_agg['freq'], func='sum')\n",
    "slot1_freqs = slot1_freqs[bigram_freqs_agg['first_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now divide the cond. frequency of 1 by marg. of 2 to get cond. prob.\n",
    "slot1_cprobs = bigram_freqs_agg['freq'] / slot2_freqs\n",
    "slot1_inf = slot1_cprobs * np.log2(slot1_cprobs)\n",
    "slot1_h = npg.aggregate(bigram_freqs_agg['second_id'], slot1_inf, func='sum')\n",
    "slot1_h = -slot1_h\n",
    "slot1_h = slot1_h[bigram_freqs_agg['second_id']]\n",
    "# Then normalize, n() is the same as the type frequency for the other slot\n",
    "slot1_h = slot1_h / np.log2(mwu_measures['type_1'])\n",
    "slot1_h = np.nan_to_num(slot1_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot2_cprobs = bigram_freqs_agg['freq'] / slot1_freqs\n",
    "slot2_inf = slot2_cprobs * np.log2(slot2_cprobs)\n",
    "slot2_h = npg.aggregate(bigram_freqs_agg['first_id'], slot2_inf, func='sum')\n",
    "slot2_h = -slot2_h\n",
    "slot2_h = slot2_h[bigram_freqs_agg['first_id']]\n",
    "slot2_h = slot2_h / np.log2(mwu_measures['type_2'])\n",
    "slot2_h = np.nan_to_num(slot2_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures['entropy_1'] = slot1_h\n",
    "mwu_measures['entropy_2'] = slot2_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigrams\n",
    "slot2_freqs = npg.aggregate(trigram_freqs_agg['second_id'], trigram_freqs_agg['freq'], func='sum')\n",
    "slot2_freqs = slot2_freqs[trigram_freqs_agg['second_id']]\n",
    "slot1_freqs = npg.aggregate(trigram_freqs_agg['first_id'], trigram_freqs_agg['freq'], func='sum')\n",
    "slot1_freqs = slot1_freqs[trigram_freqs_agg['first_id']]\n",
    "\n",
    "slot1_cprobs_tr = trigram_freqs_agg['freq'] / slot2_freqs\n",
    "slot1_inf = slot1_cprobs_tr * np.log2(slot1_cprobs_tr)\n",
    "slot1_h = npg.aggregate(trigram_freqs_agg['second_id'], slot1_inf, func='sum')\n",
    "slot1_h = -slot1_h\n",
    "slot1_h = slot1_h[trigram_freqs_agg['second_id']]\n",
    "slot1_h = slot1_h / np.log2(mwu_measures_tr['type_1'])\n",
    "slot1_h = np.nan_to_num(slot1_h)\n",
    "\n",
    "slot2_cprobs_tr = trigram_freqs_agg['freq'] / slot1_freqs\n",
    "slot2_inf = slot2_cprobs_tr * np.log2(slot2_cprobs_tr)\n",
    "slot2_h = npg.aggregate(trigram_freqs_agg['first_id'], slot2_inf, func='sum')\n",
    "slot2_h = -slot2_h\n",
    "slot2_h = slot2_h[trigram_freqs_agg['first_id']]\n",
    "slot2_h = slot2_h / np.log2(mwu_measures_tr['type_2'])\n",
    "slot2_h = np.nan_to_num(slot2_h)\n",
    "\n",
    "mwu_measures_tr['entropy_1'] = slot1_h\n",
    "mwu_measures_tr['entropy_2'] = slot2_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now both unidirectional associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot2_p = npg.aggregate(bigram_freqs_agg['second_id'], bigram_freqs_agg['freq'])\n",
    "slot1_p = npg.aggregate(bigram_freqs_agg['first_id'], bigram_freqs_agg['freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot2_p = slot2_p / np.sum(slot2_p)\n",
    "slot1_p = slot1_p / np.sum(slot1_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot2_p = slot2_p[bigram_freqs_agg['second_id']]\n",
    "slot1_p = slot1_p[bigram_freqs_agg['first_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree to which 1 attracts a following 2 is computed as the normalized KLD of the probabilities of 2 when 1 is present from the probabilities of 2 in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot2_given_slot1 = np.log2(slot2_cprobs / slot2_p)\n",
    "other2_given_slot1 = np.log2((1 - slot2_cprobs) / (1 - slot2_p))\n",
    "slot2_given_slot1 = np.nan_to_num(slot2_given_slot1, neginf=0)\n",
    "other2_given_slot1 = np.nan_to_num(other2_given_slot1, neginf=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot1_given_slot2 = np.log2(slot1_cprobs / slot1_p)\n",
    "other1_given_slot2 = np.log2((1 - slot1_cprobs) / (1 - slot1_p))\n",
    "slot1_given_slot2 = np.nan_to_num(slot1_given_slot2, neginf=0)\n",
    "other1_given_slot2 = np.nan_to_num(other1_given_slot2, neginf=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_KLD = (slot2_cprobs * slot2_given_slot1) + (1 - slot2_cprobs) * other2_given_slot1\n",
    "forward_KLD_n = 1 - np.power(np.e, -forward_KLD)\n",
    "backward_KLD = (slot1_cprobs * slot1_given_slot2) + (1 - slot1_cprobs) * other1_given_slot2\n",
    "backward_KLD_n = 1 - np.power(np.e, -backward_KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures['assoc_forw'] = forward_KLD_n\n",
    "mwu_measures['assoc_backw'] = backward_KLD_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot2_p = npg.aggregate(trigram_freqs_agg['second_id'], trigram_freqs_agg['freq'])\n",
    "slot1_p = npg.aggregate(trigram_freqs_agg['first_id'], trigram_freqs_agg['freq'])\n",
    "\n",
    "slot2_p = slot2_p / np.sum(slot2_p)\n",
    "slot1_p = slot1_p / np.sum(slot1_p)\n",
    "\n",
    "slot2_p = slot2_p[trigram_freqs_agg['second_id']]\n",
    "slot1_p = slot1_p[trigram_freqs_agg['first_id']]\n",
    "\n",
    "slot2_given_slot1 = np.log2(slot2_cprobs_tr / slot2_p)\n",
    "other2_given_slot1 = np.log2((1 - slot2_cprobs_tr) / (1 - slot2_p))\n",
    "slot2_given_slot1 = np.nan_to_num(slot2_given_slot1, neginf=0)\n",
    "other2_given_slot1 = np.nan_to_num(other2_given_slot1, neginf=0)\n",
    "\n",
    "slot1_given_slot2 = np.log2(slot1_cprobs_tr / slot1_p)\n",
    "other1_given_slot2 = np.log2((1 - slot1_cprobs_tr) / (1 - slot1_p))\n",
    "slot1_given_slot2 = np.nan_to_num(slot1_given_slot2, neginf=0)\n",
    "other1_given_slot2 = np.nan_to_num(other1_given_slot2, neginf=0)\n",
    "\n",
    "forward_KLD = (slot2_cprobs_tr * slot2_given_slot1) + (1 - slot2_cprobs_tr) * other2_given_slot1\n",
    "forward_KLD_n = 1 - np.power(np.e, -forward_KLD)\n",
    "backward_KLD = (slot1_cprobs_tr * slot1_given_slot2) + (1 - slot1_cprobs_tr) * other1_given_slot2\n",
    "backward_KLD_n = 1 - np.power(np.e, -backward_KLD)\n",
    "\n",
    "mwu_measures_tr['assoc_forw'] = forward_KLD_n\n",
    "mwu_measures_tr['assoc_backw'] = backward_KLD_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the normalization procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but first filter for token_freq > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures = mwu_measures[mwu_measures['token_freq'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures['token_freq'] = np.log(mwu_measures['token_freq'])\n",
    "mwu_measures['token_freq'] = (mwu_measures['token_freq'] - np.min(mwu_measures['token_freq'])) / (np.max(mwu_measures['token_freq']) - np.min(mwu_measures['token_freq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures['dispersion'] = (mwu_measures['dispersion'] - np.min(mwu_measures['dispersion'])) / (np.max(mwu_measures['dispersion']) - np.min(mwu_measures['dispersion']))\n",
    "mwu_measures['dispersion'] = 1 - mwu_measures['dispersion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures['type_1'] = np.log(mwu_measures['type_1'])\n",
    "mwu_measures['type_1'] = (mwu_measures['type_1'] - np.min(mwu_measures['type_1'])) / (np.max(mwu_measures['type_1']) - np.min(mwu_measures['type_1']))\n",
    "mwu_measures['type_1'] = 1 - mwu_measures['type_1']\n",
    "mwu_measures['type_2'] = np.log(mwu_measures['type_2'])\n",
    "mwu_measures['type_2'] = (mwu_measures['type_2'] - np.min(mwu_measures['type_2'])) / (np.max(mwu_measures['type_2']) - np.min(mwu_measures['type_2']))\n",
    "mwu_measures['type_2'] = 1 - mwu_measures['type_2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures['entropy_1'] = (mwu_measures['entropy_1'] - np.min(mwu_measures['entropy_1'])) / (np.max(mwu_measures['entropy_1']) - np.min(mwu_measures['entropy_1']))\n",
    "mwu_measures['entropy_1'] = 1 - mwu_measures['entropy_1']\n",
    "mwu_measures['entropy_2'] = (mwu_measures['entropy_2'] - np.min(mwu_measures['entropy_2'])) / (np.max(mwu_measures['entropy_2']) - np.min(mwu_measures['entropy_2']))\n",
    "mwu_measures['entropy_2'] = 1 - mwu_measures['entropy_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures['assoc_forw'] = (mwu_measures['assoc_forw'] - np.min(mwu_measures['assoc_forw'])) / (np.max(mwu_measures['assoc_forw']) - np.min(mwu_measures['assoc_forw']))\n",
    "mwu_measures['assoc_backw'] = (mwu_measures['assoc_backw'] - np.min(mwu_measures['assoc_backw'])) / (np.max(mwu_measures['assoc_backw']) - np.min(mwu_measures['assoc_backw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_scores_bi = np.array(mwu_measures[['token_freq', 'dispersion', 'type_1', 'type_2', 'entropy_1', 'entropy_2', 'assoc_forw', 'assoc_backw']].tolist(), dtype='f')\n",
    "mwu_scores_bi = np.mean(mwu_scores_bi, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_scores_labels = [word1 + ' ' + word2 for word1, word2 in zip(mwu_measures['first'].tolist(), mwu_measures['second'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_scores = pd.DataFrame({'bigram': mwu_scores_labels, 'mwu_score': mwu_scores_bi, 'token_freq': mwu_measures['token_freq'].tolist(), 'dispersion': mwu_measures['dispersion'].tolist(), 'type_1': mwu_measures['type_1'].tolist(), 'type_2': mwu_measures['type_2'].tolist(), 'entropy_1': mwu_measures['entropy_1'].tolist(), 'entropy_2' : mwu_measures['entropy_2'].tolist(), 'assoc_forw': mwu_measures['assoc_forw'].tolist(), 'assoc_backw': mwu_measures['assoc_backw'].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_measures_tr = mwu_measures_tr[mwu_measures_tr['token_freq'] > 1]\n",
    "\n",
    "mwu_measures_tr['token_freq'] = np.log(mwu_measures_tr['token_freq'])\n",
    "mwu_measures_tr['token_freq'] = (mwu_measures_tr['token_freq'] - np.min(mwu_measures_tr['token_freq'])) / (np.max(mwu_measures_tr['token_freq']) - np.min(mwu_measures_tr['token_freq']))\n",
    "\n",
    "mwu_measures_tr['dispersion'] = (mwu_measures_tr['dispersion'] - np.min(mwu_measures_tr['dispersion'])) / (np.max(mwu_measures_tr['dispersion']) - np.min(mwu_measures_tr['dispersion']))\n",
    "mwu_measures_tr['dispersion'] = 1 - mwu_measures_tr['dispersion']\n",
    "\n",
    "mwu_measures_tr['type_1'] = np.log(mwu_measures_tr['type_1'])\n",
    "mwu_measures_tr['type_1'] = (mwu_measures_tr['type_1'] - np.min(mwu_measures_tr['type_1'])) / (np.max(mwu_measures_tr['type_1']) - np.min(mwu_measures_tr['type_1']))\n",
    "mwu_measures_tr['type_1'] = 1 - mwu_measures_tr['type_1']\n",
    "mwu_measures_tr['type_2'] = np.log(mwu_measures_tr['type_2'])\n",
    "mwu_measures_tr['type_2'] = (mwu_measures_tr['type_2'] - np.min(mwu_measures_tr['type_2'])) / (np.max(mwu_measures_tr['type_2']) - np.min(mwu_measures_tr['type_2']))\n",
    "mwu_measures_tr['type_2'] = 1 - mwu_measures_tr['type_2']\n",
    "\n",
    "mwu_measures_tr['entropy_1'] = (mwu_measures_tr['entropy_1'] - np.min(mwu_measures_tr['entropy_1'])) / (np.max(mwu_measures_tr['entropy_1']) - np.min(mwu_measures_tr['entropy_1']))\n",
    "mwu_measures_tr['entropy_1'] = 1 - mwu_measures_tr['entropy_1']\n",
    "mwu_measures_tr['entropy_2'] = (mwu_measures_tr['entropy_2'] - np.min(mwu_measures_tr['entropy_2'])) / (np.max(mwu_measures_tr['entropy_2']) - np.min(mwu_measures_tr['entropy_2']))\n",
    "mwu_measures_tr['entropy_2'] = 1 - mwu_measures_tr['entropy_2']\n",
    "\n",
    "mwu_measures_tr['assoc_forw'] = (mwu_measures_tr['assoc_forw'] - np.min(mwu_measures_tr['assoc_forw'])) / (np.max(mwu_measures_tr['assoc_forw']) - np.min(mwu_measures_tr['assoc_forw']))\n",
    "mwu_measures_tr['assoc_backw'] = (mwu_measures_tr['assoc_backw'] - np.min(mwu_measures_tr['assoc_backw'])) / (np.max(mwu_measures_tr['assoc_backw']) - np.min(mwu_measures_tr['assoc_backw']))\n",
    "\n",
    "mwu_scores_tri = np.array(mwu_measures_tr[['token_freq', 'dispersion', 'type_1', 'type_2', 'entropy_1', 'entropy_2', 'assoc_forw', 'assoc_backw']].tolist(), dtype='f')\n",
    "mwu_scores_tri = np.mean(mwu_scores_tri, axis=1)\n",
    "\n",
    "mwu_scores_labels_tr = [word1 + ' ' + word2 for word1, word2 in zip(mwu_measures_tr['first'].tolist(), mwu_measures_tr['second'].tolist())]\n",
    "\n",
    "trigram_scores = pd.DataFrame({'trigram': mwu_scores_labels_tr, 'mwu_score': mwu_scores_tri, 'token_freq': mwu_measures_tr['token_freq'].tolist(), 'dispersion': mwu_measures_tr['dispersion'].tolist(), 'type_1': mwu_measures_tr['type_1'].tolist(), 'type_2': mwu_measures_tr['type_2'].tolist(), 'entropy_1': mwu_measures_tr['entropy_1'].tolist(), 'entropy_2' : mwu_measures_tr['entropy_2'].tolist(), 'assoc_forw': mwu_measures_tr['assoc_forw'].tolist(), 'assoc_backw': mwu_measures_tr['assoc_backw'].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_scores.to_csv('trigram_scores_bnc.csv', index=False)\n",
    "bigram_scores.to_csv('bigram_scores_bnc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mwu scores for each sentence\n",
    "\n",
    "bigram_scores = pd.read_csv('bigram_scores_bnc.csv')\n",
    "trigram_scores = pd.read_csv('trigram_scores_bnc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary with tuple to mwu scores to get the scores for each gram of the sentence\n",
    "bigram_dict = [tuple(gram.split(' ')) for gram in bigram_scores['bigram']]\n",
    "trigram_dict = [tuple(gram.split(' ')) for gram in trigram_scores['trigram']]\n",
    "bigram_dict = dict(zip(bigram_dict, bigram_scores['mwu_score']))\n",
    "trigram_dict = dict(zip(trigram_dict, trigram_scores['mwu_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_bigrams_sent = [[[bigram_dict.get(bigram, None) for bigram in sent] for sent in corpus] for corpus in all_bigrams_sent]\n",
    "included_trigrams_sent = [[[trigram_dict.get(trigram, None) for trigram in sent] for sent in corpus] for corpus in all_trigrams_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_big_mwu = [[np.array(sentence)[np.array(sentence) != np.array(None)] for sentence in corpus] for corpus in included_bigrams_sent]\n",
    "sents_trig_mwu = [[np.array(sentence)[np.array(sentence) != np.array(None)] for sentence in corpus] for corpus in included_trigrams_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_big_mwu[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.power(np.diff(sents_big_mwu[1][0]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_big_mwu_mean = [[np.mean(sentence) for sentence in corpus] for corpus in sents_big_mwu]\n",
    "sents_trig_mwu_mean = [[np.mean(sentence) for sentence in corpus] for corpus in sents_trig_mwu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_big_stab =  [[np.sqrt(np.mean(np.power(np.diff(sentence), 2))) for sentence in corpus] for corpus in sents_big_mwu]\n",
    "sents_trig_stab =  [[np.sqrt(np.mean(np.power(np.diff(sentence), 2))) for sentence in corpus] for corpus in sents_trig_mwu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_big_std =  [[np.std(sentence) if sentence.size > 0 else np.nan for sentence in corpus] for corpus in sents_big_mwu]\n",
    "sents_trig_std =  [[np.std(sentence) if sentence.size > 0 else np.nan for sentence in corpus] for corpus in sents_big_mwu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_big_max =  [[np.max(sentence) if sentence.size > 0 else np.nan for sentence in corpus] for corpus in sents_big_mwu]\n",
    "sents_trig_max =  [[np.max(sentence) if sentence.size > 0 else np.nan for sentence in corpus] for corpus in sents_trig_mwu]\n",
    "sents_big_min =  [[np.min(sentence) if sentence.size > 0 else np.nan for sentence in corpus] for corpus in sents_big_mwu]\n",
    "sents_trig_min =  [[np.min(sentence) if sentence.size > 0 else np.nan for sentence in corpus] for corpus in sents_trig_mwu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_big_n =  [[sentence.size for sentence in corpus] for corpus in sents_big_mwu]\n",
    "sents_trig_n =  [[sentence.size for sentence in corpus] for corpus in sents_trig_mwu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = [[' '.join(sentence) for sentence in corpus] for corpus in all_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_scores = [[{'sentence': sentence, 'mean_bigram': mean, 'std_bigram': std, 'stab_bigram': stab, 'max_bigram': max_mwu, 'min_bigram': min_mwu, 'n_bigram': n, 'corpus': list(corpus_cats)[i]} for sentence, mean, std, stab, max_mwu, min_mwu, n in zip(all_sentences[i], sents_big_mwu_mean[i], sents_big_std[i], sents_big_stab[i], sents_big_max[i], sents_big_min[i], sents_big_n[i])] for i in range(len(sents_big_mwu))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_scores = [[{'sentence': sentence, 'mean_trigram': mean, 'std_trigram': std, 'stab_trigram': stab, 'max_trigram': max_mwu, 'min_trigram': min_mwu, 'n_trigram': n, 'corpus': list(corpus_cats)[i]} for sentence, mean, std, stab, max_mwu, min_mwu, n in zip(all_sentences[i], sents_trig_mwu_mean[i], sents_trig_std[i], sents_trig_stab[i], sents_trig_max[i], sents_trig_min[i], sents_trig_n[i])] for i in range(len(sents_trig_mwu))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_scores_df = pd.concat([pd.DataFrame(corpus) for corpus in bigram_scores])\n",
    "trigram_scores_df = pd.concat([pd.DataFrame(corpus) for corpus in trigram_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_scores_df.to_csv('sentence_bigram_scores_bnc.csv', index=False)\n",
    "trigram_scores_df.to_csv('sentence_trigram_scores_bnc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mwu_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
